{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9c50630-e3b4-4776-96fc-200816873ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë°ì´í„° ë¡œë“œ ë° ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„± ì™„ë£Œ.\n",
      "âœ… Item Feature (ë¡œê·¸ ë³€í™˜ ë° ì´ì‚°í™” í¬í•¨) ì „ì²˜ë¦¬ ì™„ë£Œ.\n",
      "âœ… Interaction Matrix ì†ŒìŠ¤ ë°ì´í„° (ìŒì„± ìƒí˜¸ì‘ìš© í¬í•¨) ì¤€ë¹„ ì™„ë£Œ.\n",
      "âœ… User Feature Matrix ì†ŒìŠ¤ ë°ì´í„° (ë­í‚¹ ê°€ì¤‘ì¹˜ ì¦í­) ì¤€ë¹„ ì™„ë£Œ.\n",
      "\n",
      "--- LightFM í–‰ë ¬ í¬ê¸° ---\n",
      "Interaction Matrix (Total): (1037, 122)\n",
      "Interaction Matrix (Train): (1037, 122) / Non-zero: 280\n",
      "Interaction Matrix (Test): (1037, 122) / Non-zero: 70\n",
      "\n",
      "\n",
      "LightFM í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\n",
      "\n",
      "ğŸš€ **ëª¨ë¸ ê°œì„  ê²°ê³¼**\n",
      "Test Set Precision@3: 0.2524\n",
      "Test Set AUC Score: 0.9672\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from lightfm.data import Dataset\n",
    "from lightfm import LightFM\n",
    "from lightfm.evaluation import precision_at_k, auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 1. íŒŒì¼ ê²½ë¡œ ë° ìƒìˆ˜ ì •ì˜\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "FILE_PATH_MAIN = '[ìŠ¤íŠ¸ë¡±ë¼ì´í”„]ìµœì¢…_ë°ì´í„°_20250924.xlsx'\n",
    "FILE_PATH_META = \"ì œí’ˆ ë©”íƒ€ë°ì´í„° ìµœì¢….xlsx\"\n",
    "FILE_PATH_ALIGN = \"uwellnow_product_align.xlsx\" # ì œí’ˆ-ë§› ë§¤í•‘ ì •ë³´ íŒŒì¼\n",
    "\n",
    "ITEM_ID_COL = 'product_id'\n",
    "PROTEIN_COL = 'protein'\n",
    "MAX_RANK_COUNT = 7\n",
    "\n",
    "# Item Features ìƒìˆ˜ (ì•„ì´í…œ íŠ¹ì§•)\n",
    "OHE_ITEM_COLS = ['ingredient_type', 'category', 'flavor', 'sensory_tags']\n",
    "# User Features ìƒìˆ˜ (ì‚¬ìš©ì íŠ¹ì§•) - ê¸°ì¡´ê³¼ ë™ì¼\n",
    "OHE_USER_COLS = [\n",
    "    '3) ì„±ë³„', '8) ìš´ë™ í™œë™ ê¸°ê°„', '7) í”„ë¡œí‹´, í”„ë¦¬ì›Œí¬ì•„ì›ƒ, ì „í•´ì§ˆ ìŒë£Œ, ê²Œì´ë„ˆ ë“± í—¬ìŠ¤ ë³´ì¶©ì œ 2ì¢… ì´ìƒì„ ì„­ì·¨í•´ ë³´ì‹  ê²½í—˜ì´ ìˆìœ¼ì‹ ê°€ìš”?', \n",
    "    '9) ì£¼ì— ëª‡ íšŒ ì •ë„ ìš´ë™ì„ ì§„í–‰í•˜ì‹œë‚˜ìš”?(íƒ1)', '10) ì•ŒëŸ¬ì§€ ë˜ëŠ” ë¯¼ê°ì„±ë¶„(ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '11) í‰ì†Œ ì±™ê¸°ëŠ” ë¼ë‹ˆëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?(ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', \n",
    "    '12) ì‹ì‚¬ ê¸°ì¤€ìœ¼ë¡œ ìš´ë™ ì‹œê°„ì€ ì–¸ì œì¸ê°€ìš”?(íƒ 1)', '12-1) ì¼ê³¼(ìˆ˜ì—…,ì—…ë¬´,ì¼ ë“±) ê¸°ì¤€ìœ¼ë¡œ ìš´ë™ ì‹œê°„ì€ ì–¸ì œì¸ê°€ìš”?(íƒ 1)', \n",
    "    '12-2) ìš´ë™ì„ ì œì™¸í•œ ì¼ê³¼ ì¤‘ í™œë™ì€ ì–´ëŠ ì •ë„ë¡œ í™œë°œí•œê°€ìš”?(íƒ 1)', '12-3) ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ìš´ë™ ì‹œì‘ ì‹œê°„ì´ ì–¸ì œì¸ê°€ìš”?(íƒ 1)',\n",
    "    # Feature Group 2 (ì˜í–¥ ìš”ì¸ ë° ê³ ë ¤ í•­ëª©)\n",
    "    '13-3) í”„ë¡œí‹´ì˜ ì˜ì–‘ì„±ë¶„ì„ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '13-4) í”„ë¡œí‹´ì˜ íš¨ê³¼ì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '13-5) ë¸Œëœë“œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '13-6) ìœ ëª…ì¸(ì„ ìˆ˜ ë˜ëŠ” ì „ë¬¸ê°€)ì˜ ì‚¬ìš© ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '13-7) ì§€ì¸ì˜ ì‚¬ìš©ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)',\n",
    "    '14-3) í”„ë¦¬ì›Œí¬ì•„ì›ƒì˜ ì˜ì–‘ì„±ë¶„ì„ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '14-4) í”„ë¦¬ì›Œí¬ì•„ì›ƒì˜ íš¨ê³¼ì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '14-5) ë¸Œëœë“œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '14-6) ìœ ëª…ì¸(ì„ ìˆ˜ ë˜ëŠ” ì „ë¬¸ê°€)ì˜ ì‚¬ìš© ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '14-7) ì§€ì¸ì˜ ì‚¬ìš©ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ ìˆ˜íƒ ê°€ëŠ¥)',\n",
    "    '15-3) ì „í•´ì§ˆ ìŒë£Œì˜ ì˜ì–‘ì„±ë¶„ì„ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '15-4) ì „í•´ì§ˆ ìŒë£Œì˜ íš¨ê³¼ì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '15-5) ë¸Œëœë“œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '15-6) ìœ ëª…ì¸(ì„ ìˆ˜ ë˜ëŠ” ì „ë¬¸ê°€)ì˜ ì‚¬ìš© ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '15-7) ì§€ì¸ì˜ ì‚¬ìš©ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)',\n",
    "    '16-3) ê²Œì´ë„ˆì˜ ì˜ì–‘ì„±ë¶„ì„ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '16-4) ê²Œì´ë„ˆì˜ íš¨ê³¼ì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '16-5) ë¸Œëœë“œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '16-6) ìœ ëª…ì¸(ì„ ìˆ˜ ë˜ëŠ” ì „ë¬¸ê°€)ì˜ ì‚¬ìš© ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '16-7) ì§€ì¸ì˜ ì‚¬ìš©ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)',\n",
    "    '17-3) í•´ë‹¹ ë³´ì¶©ì œì˜ ì˜ì–‘ì„±ë¶„ì„ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '17-4) í•´ë‹¹ ë³´ì¶©ì œì˜ íš¨ê³¼ì—ì„œ ê³ ë ¤í•œ ì ì€ ë¬´ì—‡ì¸ê°€ìš”?', '17-5) ë¸Œëœë“œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '17-6) ìœ ëª…ì¸(ì„ ìˆ˜ ë˜ëŠ” ì „ë¬¸ê°€)ì˜ ì‚¬ìš© ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '17-7) ì§€ì¸ì˜ ì‚¬ìš©ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)',\n",
    "]\n",
    "\n",
    "# ìƒí˜¸ì‘ìš© ì»¬ëŸ¼ ì •ì˜\n",
    "INTERACTION_WEIGHT_COLS = [\n",
    "    ('13-9) [í”„ë¡œí‹´] ì„ íƒí•˜ì‹  ì œí’ˆì¤‘ì— ê°€ì¥ ì¢…í•©ì ìœ¼ë¡œ ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆ 1ê°œë§Œ ì„ íƒí•´ì£¼ì„¸ìš” (íƒ 1)', '13-10) ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆì´ ì–´ë–¤ ë§›ì¸ê°€ìš”? (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '13-17) í•´ë‹¹ í”„ë¡œí‹´ì— ëŒ€í•œ ì¬êµ¬ë§¤ ì˜ì‚¬ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”?'),\n",
    "    ('14-9) [í”„ë¦¬ì›Œí¬ì•„ì›ƒ] ì„ íƒí•˜ì‹  ì œí’ˆì¤‘ì— ê°€ì¥ ì¢…í•©ì ìœ¼ë¡œ ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆ 1ê°œë§Œ ì„ íƒí•´ì£¼ì„¸ìš” (íƒ 1)', '14-10) ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆì´ ì–´ë–¤ ë§›ì¸ê°€ìš”? (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '14-17) í•´ë‹¹ í”„ë¦¬ì›Œí¬ì•„ì›ƒì— ëŒ€í•œ ì¬êµ¬ë§¤ ì˜ì‚¬ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”?'),\n",
    "    ('15-9) [ì „í•´ì§ˆ ìŒë£Œ(BCAA, ì´ì˜¨ìŒë£Œ)] ì„ íƒí•˜ì‹  ì œí’ˆì¤‘ì— ê°€ì¥ ì¢…í•©ì ìœ¼ë¡œ ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆ 1ê°œë§Œ ì„ íƒí•´ì£¼ì„¸ìš” (íƒ 1)', '15-10) ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆì´ ì–´ë–¤ ë§›ì¸ê°€ìš”? (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '15-17) í•´ë‹¹ ì „í•´ì§ˆ ìŒë£Œì— ëŒ€í•œ ì¬êµ¬ë§¤ ì˜ì‚¬ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”?'),\n",
    "    ('16-9) [ê²Œì´ë„ˆ] ì„ íƒí•˜ì‹  ì œí’ˆì¤‘ì— ê°€ì¥ ì¢…í•©ì ìœ¼ë¡œ ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆ 1ê°œë§Œ ì„ íƒí•´ì£¼ì„¸ìš” (íƒ 1)', '16-10) ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆì´ ì–´ë–¤ ë§›ì¸ê°€ìš”? (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '16-17) í•´ë‹¹ ê²Œì´ë„ˆì— ëŒ€í•œ ì¬êµ¬ë§¤ ì˜ì‚¬ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”?'),\n",
    "    ('17-9) ì¢…í•©ì ìœ¼ë¡œ ê°€ì¥ ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆ 1ê°œë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”', '17-10) ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆì´ ì–´ë–¤ ë§›ì¸ê°€ìš”?', '17-17) í•´ë‹¹ ì œí’ˆì— ëŒ€í•œ ì¬êµ¬ë§¤ ì˜ì‚¬ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”?'),\n",
    "]\n",
    "\n",
    "RANKING_COLS_MAP = [\n",
    "    ('13-2) í”„ë¡œí‹´ ì„ íƒ ì‹œ ê°€ì¥ ì˜í–¥ì„ ë§ì´ ë°›ì€ ìš”ì†Œë“¤ì˜ ìˆœìœ„ë¥¼ ë§¤ê²¨ì£¼ì„¸ìš”', '13-17) í•´ë‹¹ í”„ë¡œí‹´ì— ëŒ€í•œ ì¬êµ¬ë§¤ ì˜ì‚¬ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”? Â ', 'PROTEIN', '13-3) í”„ë¡œí‹´ì˜ ì˜ì–‘ì„±ë¶„ì„ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)'),\n",
    "    ('14-2) í”„ë¦¬ì›Œí¬ì•„ì›ƒ ì„ íƒ ì‹œ ê°€ì¥ ì˜í–¥ì„ ë§ì´ ë°›ì€ ìš”ì†Œë“¤ì˜ ìˆœìœ„ë¥¼ ë§¤ê²¨ì£¼ì„¸ìš”', '14-17) í•´ë‹¹ í”„ë¦¬ì›Œí¬ì•„ì›ƒì— ëŒ€í•œ ì¬êµ¬ë§¤ ì˜ì‚¬ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”? Â ', 'PREWORKOUT', '14-3) í”„ë¦¬ì›Œí¬ì•„ì›ƒì˜ ì˜ì–‘ì„±ë¶„ì„ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)'),\n",
    "    ('15-2) ì „í•´ì§ˆ ìŒë£Œ(BCAA, ì´ì˜¨ìŒë£Œ)ì„ íƒ ì‹œ ê°€ì¥ ì˜í–¥ì„ ë§ì´ ë°›ì€ ìš”ì†Œë“¤ì˜ ìˆœìœ„ë¥¼ ë§¤ê²¨ì£¼ì„¸ìš”', '15-17) í•´ë‹¹ ì „í•´ì§ˆ ìŒë£Œì— ëŒ€í•œ ì¬êµ¬ë§¤ ì˜ì‚¬ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”? Â ', 'ELECTROLYTE', '15-3) ì „í•´ì§ˆ ìŒë£Œì˜ ì˜ì–‘ì„±ë¶„ì„ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)'),\n",
    "    ('16-2) ê²Œì´ë„ˆ ì„ íƒ ì‹œ ê°€ì¥ ì˜í–¥ì„ ë§ì´ ë°›ì€ ìš”ì†Œë“¤ì˜ ìˆœìœ„ë¥¼ ë§¤ê²¨ì£¼ì„¸ìš”', '16-17) í•´ë‹¹ ê²Œì´ë„ˆì— ëŒ€í•œ ì¬êµ¬ë§¤ ì˜ì‚¬ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”? Â ', 'GAINER', '16-3) ê²Œì´ë„ˆì˜ ì˜ì–‘ì„±ë¶„ì„ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)')\n",
    "]\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 2. í•„ìˆ˜ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ ì •ì˜\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "def load_and_concatenate_user_data(file_path):\n",
    "    HEADER_ROW_INDEX = 0\n",
    "    df_1st = pd.read_excel(file_path, sheet_name='1ì°¨', header=0)\n",
    "    df_2nd = pd.read_excel(file_path, sheet_name='2ì°¨', header=0)\n",
    "    df_1st.columns = df_1st.columns.astype(str).str.strip()\n",
    "    df_2nd.columns = df_2nd.columns.astype(str).str.strip()\n",
    "    \n",
    "    col_to_rename = {c: 'user_id' for c in df_2nd.columns if isinstance(c, str) and c.strip().lower() in ('no.', 'no')}\n",
    "    if col_to_rename:\n",
    "        df_2nd.rename(columns=col_to_rename, inplace=True)\n",
    "    \n",
    "    # 1ì°¨ ë°ì´í„°ì˜ 'no' ì»¬ëŸ¼ ì´ë¦„ ì •ë¦¬\n",
    "    if 'no' in df_1st.columns:\n",
    "         df_1st.rename(columns={'no': 'user_id'}, inplace=True)\n",
    "\n",
    "    df_user_raw = pd.concat([df_1st, df_2nd], ignore_index=True)\n",
    "    # ìµœì¢… user_id ì»¬ëŸ¼ ì´ë¦„ ì •ë¦¬\n",
    "    if 'no' in df_user_raw.columns and 'user_id' not in df_user_raw.columns:\n",
    "        df_user_raw.rename(columns={'no': 'user_id'}, inplace=True)\n",
    "        \n",
    "    return df_user_raw.set_index('user_id', drop=False)\n",
    "\n",
    "\n",
    "def clean_user_ids(df_user_raw):\n",
    "    df_user_raw.index = df_user_raw.index.to_series().replace('nan', np.nan) \n",
    "    valid_user_ids_numeric = pd.to_numeric(df_user_raw.index, errors='coerce')\n",
    "    # ìœ íš¨í•œ ì¸ë±ìŠ¤ (ìˆ«ìì´ë©° 0ë³´ë‹¤ í° ê°’)ë§Œ í•„í„°ë§\n",
    "    valid_indices = df_user_raw.index[valid_user_ids_numeric.notna() & (valid_user_ids_numeric > 0)].unique()\n",
    "    return df_user_raw.loc[valid_indices].copy()\n",
    "\n",
    "\n",
    "# ìˆœìœ„ì˜ ì—­ìˆœìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” í•¨ìˆ˜\n",
    "def rank_to_weight(rank_value, max_rank=MAX_RANK_COUNT):\n",
    "    rank_value = pd.to_numeric(rank_value, errors='coerce')\n",
    "    if pd.isna(rank_value) or rank_value <= 0 or rank_value > max_rank:\n",
    "        return np.nan\n",
    "    # 1ë“±ì´ ê°€ì¥ ë†’ì€ ê°€ì¤‘ì¹˜ (7), 7ë“±ì´ ê°€ì¥ ë‚®ì€ ê°€ì¤‘ì¹˜ (1)\n",
    "    return (max_rank + 1) - rank_value \n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 3. ë°ì´í„° ë¡œë“œ ë° ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„± (ITEM_FULL_ID_MAP ì •ì˜)\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "# 3-1. User Data ë¡œë“œ\n",
    "df_user_raw = load_and_concatenate_user_data(FILE_PATH_MAIN)\n",
    "df_user_clean = clean_user_ids(df_user_raw)\n",
    "    \n",
    "# 3-2. Item Data ë¡œë“œ\n",
    "df_item_raw = pd.read_excel(FILE_PATH_META, sheet_name='ì œí’ˆ ë©”íƒ€ë°ì´í„° ìµœì¢…', header=0)\n",
    "    \n",
    "# Item Data ì»¬ëŸ¼ ì´ë¦„ í´ë¦¬ë‹\n",
    "df_item_raw.columns = df_item_raw.columns.astype(str).str.strip().str.lower()\n",
    "    \n",
    "# OHE_ITEM_COLSë¥¼ ì†Œë¬¸ìë¡œ ë³€ê²½ëœ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©\n",
    "OHE_ITEM_COLS = [c.lower() for c in OHE_ITEM_COLS]\n",
    "ITEM_ID_COL = 'product_id'\n",
    "PROTEIN_COL = 'protein'\n",
    "    \n",
    "# 3-3. Item Mapping Data ë¡œë“œ ë° ITEM_FULL_ID_MAP ìƒì„±\n",
    "df_align = pd.read_excel(FILE_PATH_ALIGN, header=0)\n",
    "df_align.columns = df_align.columns.astype(str).str.strip()\n",
    "    \n",
    "df_align['MAPPING_KEY'] = (df_align['product'].astype(str).str.strip().str.upper() + \n",
    "                            '_' + \n",
    "                            df_align['flavor'].astype(str).str.strip().str.upper())\n",
    "\n",
    "ITEM_FULL_ID_MAP = pd.Series(\n",
    "    df_align['product_id'].astype(str).str.strip().str.upper().values,\n",
    "    index=df_align['MAPPING_KEY']\n",
    ").to_dict()\n",
    "    \n",
    "if 'NAN_NAN' in ITEM_FULL_ID_MAP:\n",
    "    del ITEM_FULL_ID_MAP['NAN_NAN']\n",
    "    \n",
    "print(\"âœ… ë°ì´í„° ë¡œë“œ ë° ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„± ì™„ë£Œ.\")\n",
    "    \n",
    "    \n",
    "# ---------------------------------------------------------------------------------\n",
    "# 4. Item ë§¤í•‘ í•¨ìˆ˜ ì •ì˜ (ITEM_FULL_ID_MAP ìƒì„± í›„ ì •ì˜)\n",
    "# ---------------------------------------------------------------------------------\n",
    "    \n",
    "def normalize_interaction_id(product_name_ac, flavor_ad, mapping_dict=ITEM_FULL_ID_MAP):\n",
    "    \"\"\" ì‚¬ìš©ì ì‘ë‹µ (ì œí’ˆëª…ê³¼ ë§›)ì„ ë”•ì…”ë„ˆë¦¬ í‚¤ë¡œ ì¡°í•©í•˜ì—¬ ì •í™•í•œ product_idë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤. \"\"\"\n",
    "    if not product_name_ac or not flavor_ad or not mapping_dict:\n",
    "        return None \n",
    "        \n",
    "    product_clean = str(product_name_ac).strip().upper()\n",
    "    flavor_clean = str(flavor_ad).strip().upper()\n",
    "    search_key = f\"{product_clean}_{flavor_clean}\"\n",
    "        \n",
    "    if search_key in mapping_dict:\n",
    "        return mapping_dict[search_key]\n",
    "\n",
    "    # ë§¤í•‘ë˜ì§€ ì•Šì€ ê²½ìš°, ì„ì‹œë¡œ ê²°í•©ëœ ì´ë¦„ ì‚¬ìš© (ì½œë“œ ìŠ¤íƒ€íŠ¸ ë°©ì§€)\n",
    "    combined_name = f\"{product_clean}{flavor_clean}\"\n",
    "    return combined_name.replace(' ', '').replace('-', '').replace('.', '').replace('(', '').replace(')', '')\n",
    "\n",
    "\n",
    "# df_user_cleanì˜ ì¸ë±ìŠ¤ ì´ë¦„ ì •ë¦¬ (ì˜¤ë¥˜ í•´ê²°)\n",
    "if df_user_clean.index.name == 'user_id':\n",
    "    df_user_clean.index.name = None\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "## 5ë‹¨ê³„: Item Feature (ì •ëŸ‰ í¬í•¨) ì „ì²˜ë¦¬ ë° ê°•í™” ğŸš€\n",
    "# ---------------------------------------------------------------------------------\n",
    "    \n",
    "df_item_raw[PROTEIN_COL] = pd.to_numeric(df_item_raw[PROTEIN_COL], errors='coerce')\n",
    "df_item_raw.dropna(subset=[PROTEIN_COL, ITEM_ID_COL], inplace=True)\n",
    "\n",
    "# 1. ë‹¨ë°±ì§ˆ í•¨ëŸ‰ ì´ì‚°í™” (Binning)\n",
    "bins = [0, 15, 25, df_item_raw[PROTEIN_COL].max() + 1]\n",
    "labels = ['protein_low', 'protein_mid', 'protein_high']\n",
    "df_item_raw['protein_bin'] = pd.cut(df_item_raw[PROTEIN_COL], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# 2. ë‹¨ë°±ì§ˆ í•¨ëŸ‰ ë¡œê·¸ ë³€í™˜ (ì •ëŸ‰ì  í”¼ì²˜ ê°•í™”)\n",
    "df_item_raw['protein_log'] = np.log1p(df_item_raw[PROTEIN_COL])\n",
    "\n",
    "# One-Hot Encoding í”¼ì²˜ ìƒì„±\n",
    "selected_ohe_cols = [c for c in OHE_ITEM_COLS + ['protein_bin'] if c in df_item_raw.columns]\n",
    "\n",
    "df_item_features = df_item_raw.melt(\n",
    "    id_vars=[ITEM_ID_COL], \n",
    "    value_vars=selected_ohe_cols\n",
    ")\n",
    "    \n",
    "# ê²°ì¸¡ì¹˜(NaN)ë¥¼ 'NONE' í”¼ì²˜ë¡œ ëŒ€ì²´\n",
    "df_item_features['value'] = df_item_features['value'].fillna('NONE') \n",
    "\n",
    "df_item_features['feature'] = df_item_features['variable'].astype(str) + '_' + df_item_features['value'].astype(str).str.strip()\n",
    "df_item_features = df_item_features[[ITEM_ID_COL, 'feature']].drop_duplicates()\n",
    "\n",
    "# 3. ë¡œê·¸ ë³€í™˜ëœ ë‹¨ë°±ì§ˆ í•¨ëŸ‰ì„ Item Feature Matrixì— ì¶”ê°€ (ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©)\n",
    "df_protein_log = df_item_raw.copy()\n",
    "df_protein_log['feature'] = 'protein_log_value'\n",
    "df_protein_log.rename(columns={'protein_log': 'weight'}, inplace=True)\n",
    "df_protein_log = df_protein_log[[ITEM_ID_COL, 'feature', 'weight']].copy()\n",
    "\n",
    "# ëª¨ë“  Item Features ê²°í•©\n",
    "df_item_features['weight'] = 1.0\n",
    "df_item_features = pd.concat([df_item_features[[ITEM_ID_COL, 'feature', 'weight']], df_protein_log], ignore_index=True)\n",
    "\n",
    "\n",
    "print(\"âœ… Item Feature (ë¡œê·¸ ë³€í™˜ ë° ì´ì‚°í™” í¬í•¨) ì „ì²˜ë¦¬ ì™„ë£Œ.\")\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "## 6ë‹¨ê³„: Interaction Matrix ì†ŒìŠ¤ ë°ì´í„° êµ¬ì¶• ë° ìŒì„± ìƒí˜¸ì‘ìš© ì¶”ê°€ ğŸš€\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "df_interactions_list = []\n",
    "    \n",
    "for item_col, flavor_col, weight_col in INTERACTION_WEIGHT_COLS:\n",
    "    temp_df = df_user_clean[['user_id', item_col, flavor_col, weight_col]].copy()\n",
    "    temp_df.rename(columns={item_col: 'product', flavor_col: 'flavor', weight_col: 'weight'}, inplace=True)\n",
    "    temp_df.dropna(subset=['product', 'flavor'], inplace=True)\n",
    "        \n",
    "    temp_df['item_id'] = temp_df.apply(lambda row: normalize_interaction_id(row['product'], row['flavor']), axis=1)\n",
    "    temp_df.drop(columns=['product', 'flavor'], inplace=True)\n",
    "        \n",
    "    temp_df['weight'] = pd.to_numeric(temp_df['weight'], errors='coerce')\n",
    "    \n",
    "    # ğŸš¨ ê¸ì •ì  ê°€ì¤‘ì¹˜ (ì¬êµ¬ë§¤ ì˜ì‚¬ 3ì ~5ì )\n",
    "    df_positive = temp_df[temp_df['weight'] >= 3].copy()\n",
    "    df_positive['weight'] = df_positive['weight'] * 10 # 10ë°° ì¦í­\n",
    "    df_interactions_list.append(df_positive)\n",
    "    \n",
    "    # ğŸš¨ ëª…ì‹œì  ìŒì„± ê°€ì¤‘ì¹˜ (ì¬êµ¬ë§¤ ì˜ì‚¬ 1ì ~2ì )\n",
    "    # LightFMì€ ëª…ì‹œì  í”¼ë“œë°±ì—ì„œ ìŒìˆ˜ ê°€ì¤‘ì¹˜ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ\n",
    "    df_negative = temp_df[temp_df['weight'].isin([1, 2])].copy()\n",
    "    df_negative['weight'] = -5.0 # ì¼ê´„ì ìœ¼ë¡œ ë‚®ì€ ìŒìˆ˜ ê°€ì¤‘ì¹˜ ë¶€ì—¬\n",
    "    df_interactions_list.append(df_negative)\n",
    "\n",
    "df_interactions = pd.concat(df_interactions_list, ignore_index=True)\n",
    "\n",
    "valid_items = df_item_raw[ITEM_ID_COL].unique()\n",
    "df_interactions = df_interactions[df_interactions['item_id'].isin(valid_items)]\n",
    "df_interactions.dropna(subset=['item_id', 'weight'], inplace=True)\n",
    "\n",
    "# ì¤‘ë³µ ì œê±° (ê¸ì • ìƒí˜¸ì‘ìš©ì„ ìš°ì„ , ìŒì„± ìƒí˜¸ì‘ìš©ë³´ë‹¤ ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§)\n",
    "df_interactions.sort_values(by='weight', ascending=False, inplace=True)\n",
    "df_interactions.drop_duplicates(subset=['user_id', 'item_id'], keep='first', inplace=True)\n",
    "    \n",
    "print(\"âœ… Interaction Matrix ì†ŒìŠ¤ ë°ì´í„° (ìŒì„± ìƒí˜¸ì‘ìš© í¬í•¨) ì¤€ë¹„ ì™„ë£Œ.\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "## 7ë‹¨ê³„: User Feature Matrix êµ¬ì¶• (ë­í‚¹ ê°€ì¤‘ì¹˜ ì¦í­) ğŸš€\n",
    "df_user_features_list = []\n",
    "    \n",
    "user_ohe_cols_clean = [c for c in OHE_USER_COLS if c in df_user_clean.columns]\n",
    "    \n",
    "# OHE í”¼ì²˜ (ê°€ì¤‘ì¹˜ 1.0 ìœ ì§€)\n",
    "df_user_features_ohe = df_user_clean[['user_id'] + user_ohe_cols_clean].melt(\n",
    "    id_vars='user_id', var_name='question', value_name='feature_value'\n",
    ").dropna(subset=['feature_value'])\n",
    "\n",
    "df_user_features_ohe['feature_value'] = df_user_features_ohe['feature_value'].astype(str).str.split(r'[,/]')\n",
    "df_user_features_ohe = df_user_features_ohe.explode('feature_value')\n",
    "df_user_features_ohe['feature'] = df_user_features_ohe['question'].astype(str) + '_' + df_user_features_ohe['feature_value'].astype(str).str.strip()\n",
    "df_user_features_ohe['weight'] = 1.0 \n",
    "df_user_features_list.append(df_user_features_ohe[['user_id', 'feature', 'weight']].drop_duplicates())\n",
    "\n",
    "# ë­í‚¹ í”¼ì²˜ (ê°€ì¤‘ì¹˜ ì¦í­)\n",
    "for rank_col, weight_col, product_tag, feature_start_col in RANKING_COLS_MAP:\n",
    "    if rank_col in df_user_clean.columns:\n",
    "        df_rank_user_feat = df_user_clean[['user_id', rank_col]].copy()\n",
    "        df_rank_user_feat.rename(columns={rank_col: 'rank_str'}, inplace=True)\n",
    "        df_rank_user_feat['rank_list'] = df_rank_user_feat['rank_str'].astype(str).str.strip().apply(list)\n",
    "        df_rank_user_feat = df_rank_user_feat.explode('rank_list')\n",
    "        df_rank_user_feat['feature_index'] = df_rank_user_feat.groupby('user_id').cumcount() \n",
    "            \n",
    "        all_cols = df_user_clean.columns.tolist()\n",
    "        try:\n",
    "            start_idx = all_cols.index(feature_start_col)\n",
    "            feature_cols_list = all_cols[start_idx : start_idx + MAX_RANK_COUNT] \n",
    "        except ValueError:\n",
    "            continue\n",
    "                \n",
    "        index_to_col = {i: c for i, c in enumerate(feature_cols_list)}\n",
    "            \n",
    "        df_rank_user_feat['feature_header'] = df_rank_user_feat['feature_index'].map(index_to_col)\n",
    "        df_rank_user_feat['feature'] = product_tag.upper() + '_RANK_' + df_rank_user_feat['feature_header'].astype(str).str.upper()\n",
    "        df_rank_user_feat['weight'] = df_rank_user_feat['rank_list'].apply(rank_to_weight) \n",
    "        \n",
    "        # ğŸš¨ ë­í‚¹ í”¼ì²˜ ê°€ì¤‘ì¹˜ ì¦í­ (7ë°°) \n",
    "        df_rank_user_feat['weight'] = df_rank_user_feat['weight'] * 7 \n",
    "        \n",
    "        df_rank_user_feat.dropna(subset=['feature', 'weight'], inplace=True)\n",
    "        df_user_features_list.append(df_rank_user_feat[['user_id', 'feature', 'weight']].drop_duplicates())\n",
    "\n",
    "df_user_features = pd.concat(df_user_features_list, ignore_index=True)\n",
    "df_user_features = df_user_features.drop_duplicates(subset=['user_id', 'feature'], keep='first')\n",
    "    \n",
    "print(\"âœ… User Feature Matrix ì†ŒìŠ¤ ë°ì´í„° (ë­í‚¹ ê°€ì¤‘ì¹˜ ì¦í­) ì¤€ë¹„ ì™„ë£Œ.\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "## 8ë‹¨ê³„: LightFM Dataset êµ¬ì¶• ë° Train/Test ë¶„ë¦¬ \n",
    "# ---------------------------------------------------------------------------------\n",
    "    \n",
    "dataset = Dataset()\n",
    "dataset.fit(\n",
    "    users=df_user_clean.index.unique(),\n",
    "    items=df_item_raw[ITEM_ID_COL].unique(),\n",
    "    user_features=df_user_features['feature'].unique(),\n",
    "    item_features=df_item_features['feature'].unique()\n",
    ")\n",
    "    \n",
    "(interactions_all, sample_weights_all) = dataset.build_interactions(\n",
    "    (row['user_id'], row['item_id'], row['weight'])\n",
    "    for index, row in df_interactions.iterrows()\n",
    ")\n",
    "    \n",
    "total_indices = np.arange(len(df_interactions))\n",
    "# np.random.seed(42) # ğŸš¨ ì‹œë“œ ì œê±°/ì£¼ì„ ì²˜ë¦¬\n",
    "np.random.shuffle(total_indices)\n",
    "    \n",
    "test_size = int(len(df_interactions) * 0.2)\n",
    "train_indices = total_indices[test_size:]\n",
    "test_indices = total_indices[:test_size]\n",
    "    \n",
    "df_train_interactions = df_interactions.iloc[train_indices].copy()\n",
    "df_test_interactions = df_interactions.iloc[test_indices].copy()\n",
    "    \n",
    "# LightFM Dataset.build_interactionsë¥¼ ì‚¬ìš©í•˜ì—¬ í–‰ë ¬ êµ¬ì¶•\n",
    "(interactions_train, weights_train) = dataset.build_interactions(\n",
    "    (row['user_id'], row['item_id'], row['weight']) for index, row in df_train_interactions.iterrows()\n",
    ")\n",
    "(interactions_test, weights_test) = dataset.build_interactions(\n",
    "    (row['user_id'], row['item_id'], row['weight']) for index, row in df_test_interactions.iterrows()\n",
    ")\n",
    "    \n",
    "# LightFM Feature Matrix êµ¬ì¶•\n",
    "user_features_matrix = dataset.build_user_features(\n",
    "    (row['user_id'], {row['feature']: row['weight']}) for index, row in df_user_features.iterrows()\n",
    ")\n",
    "# Item Feature Matrix êµ¬ì¶• (ê°€ì¤‘ì¹˜ 1.0 ë° ë¡œê·¸ ë³€í™˜ ê°€ì¤‘ì¹˜ í¬í•¨)\n",
    "item_features_matrix = dataset.build_item_features(\n",
    "    (row[ITEM_ID_COL], {row['feature']: row['weight']}) for index, row in df_item_features.iterrows()\n",
    ")\n",
    "    \n",
    "print(\"\\n--- LightFM í–‰ë ¬ í¬ê¸° ---\")\n",
    "print(f\"Interaction Matrix (Total): {interactions_all.shape}\")\n",
    "print(f\"Interaction Matrix (Train): {interactions_train.shape} / Non-zero: {interactions_train.getnnz()}\")\n",
    "print(f\"Interaction Matrix (Test): {interactions_test.shape} / Non-zero: {interactions_test.getnnz()}\")\n",
    "    \n",
    "    \n",
    "# ---------------------------------------------------------------------------------\n",
    "## 9ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ (Train Set ì‚¬ìš©)\n",
    "    \n",
    "model = LightFM(\n",
    "    loss='warp', \n",
    "    no_components=60, # ğŸš¨ ì»´í¬ë„ŒíŠ¸ ìˆ˜ ì¦ê°€ (50 -> 60)\n",
    "    learning_rate=0.03,\n",
    "    user_alpha=1e-6,  # ğŸš¨ L2 ì •ê·œí™”(Alpha) ì¶”ê°€ (ì˜¤ë²„í”¼íŒ… ë°©ì§€)\n",
    "    item_alpha=1e-6,\n",
    "    # random_state=42 # ğŸš¨ ì‹œë“œ ì œê±°/ì£¼ì„ ì²˜ë¦¬\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    interactions_train, \n",
    "    sample_weight=weights_train, \n",
    "    user_features=user_features_matrix, \n",
    "    item_features=item_features_matrix, \n",
    "    epochs=100, # ğŸš¨ Epochs ì¦ê°€ (60 -> 100)\n",
    "    num_threads=4,\n",
    ")\n",
    "\n",
    "print(\"\\n\\nLightFM í•˜ì´ë¸Œë¦¬ë“œ ì¶”ì²œ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\")\n",
    "    \n",
    "# ---------------------------------------------------------------------------------\n",
    "## 10ë‹¨ê³„: ëª¨ë¸ í‰ê°€\n",
    "test_precision = precision_at_k(\n",
    "    model, \n",
    "    interactions_test, \n",
    "    user_features=user_features_matrix, \n",
    "    item_features=item_features_matrix, \n",
    "    k=3\n",
    "    # check_matching=True ì¸ìˆ˜ë¥¼ ì œê±°í•©ë‹ˆë‹¤.\n",
    ").mean()\n",
    "\n",
    "test_auc = auc_score(\n",
    "    model, \n",
    "    interactions_test, \n",
    "    user_features=user_features_matrix, \n",
    "    item_features=item_features_matrix\n",
    ").mean()\n",
    "\n",
    "print(f\"\\nğŸš€ **ëª¨ë¸ ê°œì„  ê²°ê³¼**\")\n",
    "print(f\"Test Set Precision@3: {test_precision:.4f}\")\n",
    "print(f\"Test Set AUC Score: {test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028040a8-8c60-48fa-bbc4-46acf26cf707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70563e7c-9ec3-412c-9cf9-c51840d053f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (LightFM Ready)",
   "language": "python",
   "name": "lightfm_python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
