{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "770a5acd-43c9-4970-96fd-d6c9dfe4ad33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /opt/homebrew/Caskroom/miniforge/base/envs/lightfm_python311/lib/python3.11/site-packages (3.1.0)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniforge/base/envs/lightfm_python311/lib/python3.11/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/homebrew/Caskroom/miniforge/base/envs/lightfm_python311/lib/python3.11/site-packages (from xgboost) (1.16.2)\n",
      "ì„¤ì¹˜ ì™„ë£Œ. ì»¤ë„ ì¬ì‹œì‘ í›„ ì½”ë“œë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•´ ì£¼ì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install xgboost\n",
    "print(\"ì„¤ì¹˜ ì™„ë£Œ. ì»¤ë„ ì¬ì‹œì‘ í›„ ì½”ë“œë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•´ ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e06bb68-819d-4942-b40a-c9f9fe5d78e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd1a107c-8558-4497-a3ed-79f42cd6c6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ë°ì´í„° ë¡œë“œ ë° í´ë¦¬ë‹ ì™„ë£Œ.\n",
      "Item í”¼ì²˜ ê°œìˆ˜: 134\n",
      "User í”¼ì²˜ ê°œìˆ˜: 389\n",
      "\n",
      "âŒ ìµœì¢… ëª¨ë¸ êµ¬ì¶• ì¤‘ ì˜¤ë¥˜ ë°œìƒ: 'item_id'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import os\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 1. íŒŒì¼ ê²½ë¡œ ë° ìƒìˆ˜ ì •ì˜ (ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ë³€ê²½ í•„ìš”)\n",
    "# ---------------------------------------------------------------------------------\n",
    "# ğŸš¨ ì£¼ì˜: ì•„ë˜ íŒŒì¼ ê²½ë¡œëŠ” ì‹¤ì œ í™˜ê²½ì— ë§ê²Œ ë³€ê²½í•´ì•¼ í•©ë‹ˆë‹¤. \n",
    "FILE_PATH_MAIN = '[ìŠ¤íŠ¸ë¡±ë¼ì´í”„]ìµœì¢…_ë°ì´í„°_20250924.xlsx'\n",
    "FILE_PATH_META = \"ì œí’ˆ ë©”íƒ€ë°ì´í„° ìµœì¢….xlsx\"\n",
    "FILE_PATH_ALIGN = \"uwellnow_product_align.xlsx\" \n",
    "\n",
    "ITEM_ID_COL = 'product_id'\n",
    "PROTEIN_COL = 'protein'\n",
    "MAX_RANK_COUNT = 7 # ë­í‚¹ ë°ì´í„°ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ ìƒìˆ˜ ìœ ì§€\n",
    "\n",
    "# OHE Item Features (ì•„ì´í…œ íŠ¹ì§•)\n",
    "OHE_ITEM_COLS = ['ingredient_type', 'category', 'flavor', 'sensory_tags']\n",
    "# OHE User Features (ì‚¬ìš©ì íŠ¹ì§•)\n",
    "OHE_USER_COLS = [\n",
    "    '3) ì„±ë³„', '8) ìš´ë™ í™œë™ ê¸°ê°„', '7) í”„ë¡œí‹´, í”„ë¦¬ì›Œí¬ì•„ì›ƒ, ì „í•´ì§ˆ ìŒë£Œ, ê²Œì´ë„ˆ ë“± í—¬ìŠ¤ ë³´ì¶©ì œ 2ì¢… ì´ìƒì„ ì„­ì·¨í•´ ë³´ì‹  ê²½í—˜ì´ ìˆìœ¼ì‹ ê°€ìš”?', \n",
    "    '9) ì£¼ì— ëª‡ íšŒ ì •ë„ ìš´ë™ì„ ì§„í–‰í•˜ì‹œë‚˜ìš”?(íƒ1)', '10) ì•ŒëŸ¬ì§€ ë˜ëŠ” ë¯¼ê°ì„±ë¶„(ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '11) í‰ì†Œ ì±™ê¸°ëŠ” ë¼ë‹ˆëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?(ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', \n",
    "    '12) ì‹ì‚¬ ê¸°ì¤€ìœ¼ë¡œ ìš´ë™ ì‹œê°„ì€ ì–¸ì œì¸ê°€ìš”?(íƒ 1)', '12-1) ì¼ê³¼(ìˆ˜ì—…,ì—…ë¬´,ì¼ ë“±) ê¸°ì¤€ìœ¼ë¡œ ìš´ë™ ì‹œê°„ì€ ì–¸ì œì¸ê°€ìš”?(íƒ 1)', \n",
    "    '12-2) ìš´ë™ì„ ì œì™¸í•œ ì¼ê³¼ ì¤‘ í™œë™ì€ ì–´ëŠ ì •ë„ë¡œ í™œë°œí•œê°€ìš”?(íƒ 1)', '12-3) ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ìš´ë™ ì‹œì‘ ì‹œê°„ì´ ì–¸ì œì¸ê°€ìš”?(íƒ 1)',\n",
    "    # Feature Group 2 (ì˜í–¥ ìš”ì¸ ë° ê³ ë ¤ í•­ëª©) - LightFMì—ì„œ ì‚¬ìš©í–ˆìœ¼ë‚˜, ì—¬ê¸°ì„œëŠ” OHE_USER_COLSì— í¬í•¨í•˜ì—¬ í™œìš©í•©ë‹ˆë‹¤.\n",
    "    '13-3) í”„ë¡œí‹´ì˜ ì˜ì–‘ì„±ë¶„ì„ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '13-4) í”„ë¡œí‹´ì˜ íš¨ê³¼ì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '13-5) ë¸Œëœë“œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '13-6) ìœ ëª…ì¸(ì„ ìˆ˜ ë˜ëŠ” ì „ë¬¸ê°€)ì˜ ì‚¬ìš© ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '13-7) ì§€ì¸ì˜ ì‚¬ìš©ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)',\n",
    "    '14-3) í”„ë¦¬ì›Œí¬ì•„ì›ƒì˜ ì˜ì–‘ì„±ë¶„ì„ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '14-4) í”„ë¦¬ì›Œí¬ì•„ì›ƒì˜ íš¨ê³¼ì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '14-5) ë¸Œëœë“œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '14-6) ìœ ëª…ì¸(ì„ ìˆ˜ ë˜ëŠ” ì „ë¬¸ê°€)ì˜ ì‚¬ìš© ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '14-7) ì§€ì¸ì˜ ì‚¬ìš©ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ ìˆ˜íƒ ê°€ëŠ¥)',\n",
    "    '15-3) ì „í•´ì§ˆ ìŒë£Œì˜ ì˜ì–‘ì„±ë¶„ì„ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '15-4) ì „í•´ì§ˆ ìŒë£Œì˜ íš¨ê³¼ì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '15-5) ë¸Œëœë“œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '15-6) ìœ ëª…ì¸(ì„ ìˆ˜ ë˜ëŠ” ì „ë¬¸ê°€)ì˜ ì‚¬ìš© ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '15-7) ì§€ì¸ì˜ ì‚¬ìš©ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)',\n",
    "    '16-3) ê²Œì´ë„ˆì˜ ì˜ì–‘ì„±ë¶„ì„ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '16-4) ê²Œì´ë„ˆì˜ íš¨ê³¼ì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '16-5) ë¸Œëœë“œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '16-6) ìœ ëª…ì¸(ì„ ìˆ˜ ë˜ëŠ” ì „ë¬¸ê°€)ì˜ ì‚¬ìš© ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '16-7) ì§€ì¸ì˜ ì‚¬ìš©ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)',\n",
    "    '17-3) í•´ë‹¹ ë³´ì¶©ì œì˜ ì˜ì–‘ì„±ë¶„ì„ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '17-4) í•´ë‹¹ ë³´ì¶©ì œì˜ íš¨ê³¼ì—ì„œ ê³ ë ¤í•œ ì ì€ ë¬´ì—‡ì¸ê°€ìš”?', '17-5) ë¸Œëœë“œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '17-6) ìœ ëª…ì¸(ì„ ìˆ˜ ë˜ëŠ” ì „ë¬¸ê°€)ì˜ ì‚¬ìš© ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '17-7) ì§€ì¸ì˜ ì‚¬ìš©ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)',\n",
    "]\n",
    "\n",
    "# ìƒí˜¸ì‘ìš© ì»¬ëŸ¼ ì •ì˜ (ì¬êµ¬ë§¤ ì˜ì‚¬ ì ìˆ˜ë¥¼ íƒ€ê²Ÿ ë³€ìˆ˜ë¡œ ì‚¬ìš©)\n",
    "INTERACTION_WEIGHT_COLS = [\n",
    "    ('13-9) [í”„ë¡œí‹´] ì„ íƒí•˜ì‹  ì œí’ˆì¤‘ì— ê°€ì¥ ì¢…í•©ì ìœ¼ë¡œ ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆ 1ê°œë§Œ ì„ íƒí•´ì£¼ì„¸ìš” (íƒ 1)', '13-10) ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆì´ ì–´ë–¤ ë§›ì¸ê°€ìš”? (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '13-17) í•´ë‹¹ í”„ë¡œí‹´ì— ëŒ€í•œ ì¬êµ¬ë§¤ ì˜ì‚¬ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”?'),\n",
    "    ('14-9) [í”„ë¦¬ì›Œí¬ì•„ì›ƒ] ì„ íƒí•˜ì‹  ì œí’ˆì¤‘ì— ê°€ì¥ ì¢…í•©ì ìœ¼ë¡œ ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆ 1ê°œë§Œ ì„ íƒí•´ì£¼ì„¸ìš” (íƒ 1)', '14-10) ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆì´ ì–´ë–¤ ë§›ì¸ê°€ìš”? (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '14-17) í•´ë‹¹ í”„ë¦¬ì›Œí¬ì•„ì›ƒì— ëŒ€í•œ ì¬êµ¬ë§¤ ì˜ì‚¬ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”?'),\n",
    "    ('15-9) [ì „í•´ì§ˆ ìŒë£Œ(BCAA, ì´ì˜¨ìŒë£Œ)] ì„ íƒí•˜ì‹  ì œí’ˆì¤‘ì— ê°€ì¥ ì¢…í•©ì ìœ¼ë¡œ ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆ 1ê°œë§Œ ì„ íƒí•´ì£¼ì„¸ìš” (íƒ 1)', '15-10) ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆì´ ì–´ë–¤ ë§›ì¸ê°€ìš”? (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '15-17) í•´ë‹¹ ì „í•´ì§ˆ ìŒë£Œì— ëŒ€í•œ ì¬êµ¬ë§¤ ì˜ì‚¬ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”?'),\n",
    "    ('16-9) [ê²Œì´ë„ˆ] ì„ íƒí•˜ì‹  ì œí’ˆì¤‘ì— ê°€ì¥ ì¢…í•©ì ìœ¼ë¡œ ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆ 1ê°œë§Œ ì„ íƒí•´ì£¼ì„¸ìš” (íƒ 1)', '16-10) ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆì´ ì–´ë–¤ ë§›ì¸ê°€ìš”? (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)', '16-17) í•´ë‹¹ ê²Œì´ë„ˆì— ëŒ€í•œ ì¬êµ¬ë§¤ ì˜ì‚¬ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”?'),\n",
    "    ('17-9) ì¢…í•©ì ìœ¼ë¡œ ê°€ì¥ ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆ 1ê°œë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”', '17-10) ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆì´ ì–´ë–¤ ë§›ì¸ê°€ìš”?', '17-17) í•´ë‹¹ ì œí’ˆì— ëŒ€í•œ ì¬êµ¬ë§¤ ì˜ì‚¬ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”?'),\n",
    "]\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 2. í•„ìˆ˜ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ ì •ì˜ (ì´ì „ ì½”ë“œ ì¬ì‚¬ìš©)\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "def load_and_concatenate_user_data(file_path):\n",
    "    # ... (í•¨ìˆ˜ ë‚´ìš© ìœ ì§€) ...\n",
    "    HEADER_ROW_INDEX = 0\n",
    "    # íŒŒì¼ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ì§€ ì•Šì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ Mockup Data\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"ê²½ê³ : íŒŒì¼ ê²½ë¡œ '{file_path}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Mock Dataë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "        return pd.DataFrame({'user_id': range(1, 10), '3) ì„±ë³„': ['ë‚¨', 'ì—¬'] * 5})[:9].set_index('user_id', drop=False)\n",
    "        \n",
    "    df_1st = pd.read_excel(file_path, sheet_name='1ì°¨', header=0)\n",
    "    df_2nd = pd.read_excel(file_path, sheet_name='2ì°¨', header=0)\n",
    "    df_1st.columns = df_1st.columns.astype(str).str.strip()\n",
    "    df_2nd.columns = df_2nd.columns.astype(str).str.strip()\n",
    "    col_to_rename = {c: 'no' for c in df_2nd.columns if isinstance(c, str) and c.strip() == 'no.'}\n",
    "    if col_to_rename:\n",
    "        df_2nd.rename(columns=col_to_rename, inplace=True)\n",
    "    df_user_raw = pd.concat([df_1st, df_2nd], ignore_index=True)\n",
    "    df_user_raw.rename(columns={'no': 'user_id'}, inplace=True)\n",
    "    return df_user_raw.set_index('user_id', drop=False)\n",
    "\n",
    "\n",
    "def clean_user_ids(df_user_raw):\n",
    "    # ... (í•¨ìˆ˜ ë‚´ìš© ìœ ì§€) ...\n",
    "    df_user_raw.index = df_user_raw.index.to_series().replace('nan', np.nan) \n",
    "    valid_user_ids_numeric = pd.to_numeric(df_user_raw.index, errors='coerce')\n",
    "    valid_indices = df_user_raw.index[valid_user_ids_numeric.notna() & (valid_user_ids_numeric > 0)].unique()\n",
    "    return df_user_raw.loc[valid_indices].copy()\n",
    "\n",
    "\n",
    "def normalize_interaction_id(product_name_ac, flavor_ad, mapping_dict):\n",
    "    \"\"\" ì‚¬ìš©ì ì‘ë‹µ (ì œí’ˆëª…ê³¼ ë§›)ì„ ë”•ì…”ë„ˆë¦¬ í‚¤ë¡œ ì¡°í•©í•˜ì—¬ ì •í™•í•œ product_idë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤. \"\"\"\n",
    "    if not product_name_ac or not flavor_ad or not mapping_dict:\n",
    "        return None \n",
    "        \n",
    "    product_clean = str(product_name_ac).strip().upper()\n",
    "    flavor_clean = str(flavor_ad).strip().upper()\n",
    "    search_key = f\"{product_clean}_{flavor_clean}\"\n",
    "        \n",
    "    if search_key in mapping_dict:\n",
    "        return mapping_dict[search_key]\n",
    "\n",
    "    combined_name = f\"{product_clean}{flavor_clean}\"\n",
    "    return combined_name.replace(' ', '').replace('-', '').replace('.', '').replace('(', '').replace(')', '')\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 3. XGBoostë¥¼ ìœ„í•œ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "def preprocess_for_xgboost(df_user_clean, df_item_raw, df_align):\n",
    "    \n",
    "    # 3-1. Item Mapping Data ë¡œë“œ ë° ITEM_FULL_ID_MAP ìƒì„±\n",
    "    df_align.columns = df_align.columns.astype(str).str.strip()\n",
    "    df_align['MAPPING_KEY'] = (df_align['product'].astype(str).str.strip().str.upper() + \n",
    "                                '_' + \n",
    "                                df_align['flavor'].astype(str).str.strip().str.upper())\n",
    "\n",
    "    ITEM_FULL_ID_MAP = pd.Series(\n",
    "        df_align['product_id'].astype(str).str.strip().str.upper().values,\n",
    "        index=df_align['MAPPING_KEY']\n",
    "    ).to_dict()\n",
    "    \n",
    "    if 'NAN_NAN' in ITEM_FULL_ID_MAP:\n",
    "        del ITEM_FULL_ID_MAP['NAN_NAN']\n",
    "\n",
    "\n",
    "    # 3-2. Interaction Matrix ì†ŒìŠ¤ ë°ì´í„° êµ¬ì¶• (íƒ€ê²Ÿ ë³€ìˆ˜ Y)\n",
    "    df_interactions_list = []\n",
    "    \n",
    "    for item_col, flavor_col, weight_col in INTERACTION_WEIGHT_COLS:\n",
    "        temp_df = df_user_clean[['user_id', item_col, flavor_col, weight_col]].copy()\n",
    "        temp_df.rename(columns={item_col: 'product', flavor_col: 'flavor', weight_col: 'weight'}, inplace=True)\n",
    "        temp_df.dropna(subset=['product', 'flavor'], inplace=True)\n",
    "        \n",
    "        # product_id ë§¤í•‘ ë° í´ë¦¬ë‹\n",
    "        temp_df['item_id'] = temp_df.apply(lambda row: normalize_interaction_id(row['product'], row['flavor'], ITEM_FULL_ID_MAP), axis=1)\n",
    "        temp_df.drop(columns=['product', 'flavor'], inplace=True)\n",
    "        \n",
    "        # íƒ€ê²Ÿ ë³€ìˆ˜ (ì¬êµ¬ë§¤ ì˜ì‚¬ ì ìˆ˜ 1-5)\n",
    "        temp_df['weight'] = pd.to_numeric(temp_df['weight'], errors='coerce')\n",
    "        # LightFMì²˜ëŸ¼ ì¦í­í•˜ì§€ ì•Šê³  1-5ì  ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "        \n",
    "        df_interactions_list.append(temp_df)\n",
    "    \n",
    "    df_interactions = pd.concat(df_interactions_list, ignore_index=True)\n",
    "    \n",
    "    df_interactions.columns = df_interactions.columns.astype(str).str.strip().str.lower()\n",
    "    \n",
    "    valid_items = df_item_raw[ITEM_ID_COL].unique()\n",
    "    df_interactions = df_interactions[df_interactions['item_id'].isin(valid_items)]\n",
    "    df_interactions.dropna(subset=['item_id', 'weight'], inplace=True)\n",
    "    df_interactions.sort_values(by='weight', ascending=False, inplace=True)\n",
    "    df_interactions.drop_duplicates(subset=['user_id', 'item_id'], keep='first', inplace=True)\n",
    "    \n",
    "    # 3-3. Item Feature ì¤€ë¹„ ë° One-Hot Encoding\n",
    "    df_item_raw['protein'] = pd.to_numeric(df_item_raw['protein'], errors='coerce')\n",
    "    df_item_raw.dropna(subset=['protein', ITEM_ID_COL], inplace=True)\n",
    "    bins = [0, 15, 25, df_item_raw['protein'].max() + 1]\n",
    "    labels = ['protein_low', 'protein_mid', 'protein_high']\n",
    "    df_item_raw['protein_bin'] = pd.cut(df_item_raw['protein'], bins=bins, labels=labels, right=False).astype(str)\n",
    "    \n",
    "    selected_ohe_item_cols = [c for c in OHE_ITEM_COLS + ['protein_bin'] if c in df_item_raw.columns]\n",
    "    \n",
    "    df_item_features = df_item_raw[[ITEM_ID_COL] + selected_ohe_item_cols].copy()\n",
    "    \n",
    "    # ì—¬ëŸ¬ í”¼ì²˜ ì»¬ëŸ¼ì„ í•˜ë‚˜ì˜ í”¼ì²˜ë¡œ í†µí•© (One-Hot Encoding ì¤€ë¹„)\n",
    "    df_item_long = df_item_features.melt(id_vars=[ITEM_ID_COL], value_vars=selected_ohe_item_cols, value_name='feature_value')\n",
    "    df_item_long.dropna(subset=['feature_value'], inplace=True)\n",
    "    df_item_long['feature_value'] = df_item_long['feature_value'].astype(str).str.split(r'[,/]')\n",
    "    df_item_long = df_item_long.explode('feature_value')\n",
    "    \n",
    "    df_item_features_ohe = pd.get_dummies(df_item_long, columns=['feature_value'], prefix='item_feat', prefix_sep='_')\n",
    "    \n",
    "    # OHE ê²°ê³¼ë¥¼ Item ID ê¸°ì¤€ìœ¼ë¡œ í•©ì‚°í•˜ì—¬ Feature Matrix ìƒì„±\n",
    "    item_cols_to_sum = [col for col in df_item_features_ohe.columns if col.startswith('item_feat_')]\n",
    "    df_item_pivot = df_item_features_ohe.groupby(ITEM_ID_COL)[item_cols_to_sum].sum().reset_index()\n",
    "    \n",
    "    # 3-4. User Feature ì¤€ë¹„ ë° One-Hot Encoding (ë­í‚¹ ë°ì´í„° ì œì™¸)\n",
    "    user_ohe_cols_clean = [c for c in OHE_USER_COLS if c in df_user_clean.columns]\n",
    "    \n",
    "    df_user_long = df_user_clean[['user_id'] + user_ohe_cols_clean].melt(\n",
    "        id_vars='user_id', var_name='question', value_name='feature_value'\n",
    "    ).dropna(subset=['feature_value'])\n",
    "\n",
    "    df_user_long['feature_value'] = df_user_long['feature_value'].astype(str).str.split(r'[,/]')\n",
    "    df_user_long = df_user_long.explode('feature_value')\n",
    "    \n",
    "    df_user_features_ohe = pd.get_dummies(df_user_long, columns=['feature_value'], prefix='user_feat', prefix_sep='_')\n",
    "\n",
    "    # OHE ê²°ê³¼ë¥¼ User ID ê¸°ì¤€ìœ¼ë¡œ í•©ì‚°í•˜ì—¬ Feature Matrix ìƒì„±\n",
    "    user_cols_to_sum = [col for col in df_user_features_ohe.columns if col.startswith('user_feat_')]\n",
    "    df_user_pivot = df_user_features_ohe.groupby('user_id')[user_cols_to_sum].sum().reset_index()\n",
    "\n",
    "    print(f\"Item í”¼ì²˜ ê°œìˆ˜: {len(item_cols_to_sum)}\")\n",
    "    print(f\"User í”¼ì²˜ ê°œìˆ˜: {len(user_cols_to_sum)}\")\n",
    "\n",
    "    # 3-5. Interaction, User, Item Feature í†µí•© (XGBoost í›ˆë ¨ ë°ì´í„°ì…‹)\n",
    "    df_train_data = df_interactions.merge(df_user_pivot, on='user_id', how='left')\n",
    "    df_train_data = df_train_data.merge(df_item_pivot, on='item_id', how='left')\n",
    "\n",
    "    # í”¼ì²˜ê°€ ì—†ëŠ” í–‰ (Merge ì‹¤íŒ¨ ë˜ëŠ” ê²°ì¸¡ì¹˜) ì œê±°\n",
    "    df_train_data.dropna(subset=['weight'], inplace=True)\n",
    "    df_train_data = df_train_data.fillna(0) # ê²°ì¸¡ëœ í”¼ì²˜ëŠ” 0ìœ¼ë¡œ ì±„ì›€\n",
    "    \n",
    "    print(f\"XGBoost ìµœì¢… í•™ìŠµ ë°ì´í„° í¬ê¸°: {df_train_data.shape}\")\n",
    "    \n",
    "    return df_train_data, df_item_raw\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 4. ì¶”ì²œ ì‹œìŠ¤í…œ í‰ê°€ í•¨ìˆ˜ ì •ì˜ (RMSE, Precision@K)\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "def precision_at_k_recommender(model, df_test_full, k=3):\n",
    "    \"\"\"\n",
    "    XGBoost ëª¨ë¸ì˜ ì˜ˆì¸¡ ì ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ì²œ ì •ë°€ë„(Precision@K)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "    (í˜‘ì—… í•„í„°ë§ í‰ê°€ ë°©ì‹ì„ ì°¨ìš©)\n",
    "    \"\"\"\n",
    "    # 1. í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë¶€í„° ì‚¬ìš©ìê°€ ì‹¤ì œë¡œ ìƒí˜¸ì‘ìš©í•œ ì•„ì´í…œì„ ì¶”ì¶œ\n",
    "    actual_interactions = defaultdict(set)\n",
    "    # ğŸš¨ df_test_fullì€ ì´ë¯¸ user_id, item_id, weightë¥¼ ê°€ì§€ê³  ìˆìŒ\n",
    "    for index, row in df_test_full.iterrows():\n",
    "        actual_interactions[row['user_id']].add(row['item_id'])\n",
    "    \n",
    "    # 2. ëª¨ë“  ê°€ëŠ¥í•œ user-item ìŒì„ ìƒì„±í•˜ì—¬ ì˜ˆì¸¡\n",
    "    unique_users = df_test_full['user_id'].unique()\n",
    "    \n",
    "    # ğŸš¨ ì˜ˆì¸¡ì„ ìœ„í•´ í”¼ì²˜ë§Œ ì‚¬ìš©\n",
    "    X_test_only_features = df_test_full.drop(columns=['user_id', 'item_id', 'weight'])\n",
    "    \n",
    "    df_test_full['prediction'] = model.predict(X_test_only_features)\n",
    "    \n",
    "    # 3. ì‚¬ìš©ìë³„ ìƒìœ„ Kê°œ ì¶”ì²œ ê³„ì‚°\n",
    "    total_precision = []\n",
    "    \n",
    "    for user in unique_users:\n",
    "        # í•´ë‹¹ ì‚¬ìš©ìì˜ ëª¨ë“  ì˜ˆì¸¡ ì ìˆ˜ ê°€ì ¸ì˜¤ê¸°\n",
    "        user_preds = df_test_full[df_test_full['user_id'] == user].sort_values(\n",
    "            by='prediction', ascending=False\n",
    "        )\n",
    "        \n",
    "        # ìƒìœ„ Kê°œ ì•„ì´í…œ (ì˜ˆì¸¡ ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ kê°œ)\n",
    "        top_k_recommended = set(user_preds['item_id'].head(k))\n",
    "        \n",
    "        # ì‹¤ì œë¡œ ìƒí˜¸ì‘ìš©í•œ ì•„ì´í…œ\n",
    "        user_actual = actual_interactions[user]\n",
    "        \n",
    "        # ì •ë°€ë„ ê³„ì‚°: (ì‹¤ì œ ìƒí˜¸ì‘ìš© & ì¶”ì²œëœ ì•„ì´í…œ) / K\n",
    "        if len(top_k_recommended) > 0:\n",
    "            hits = len(top_k_recommended.intersection(user_actual))\n",
    "            precision = hits / k\n",
    "            total_precision.append(precision)\n",
    "            \n",
    "    return np.mean(total_precision)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 5. ë©”ì¸ ì‹¤í–‰ ë¸”ë¡\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # ë°ì´í„° ë¡œë“œ\n",
    "        df_user_raw = load_and_concatenate_user_data(FILE_PATH_MAIN)\n",
    "        df_user_clean = clean_user_ids(df_user_raw)\n",
    "        \n",
    "        df_item_raw = pd.read_excel(FILE_PATH_META, sheet_name='ì œí’ˆ ë©”íƒ€ë°ì´í„° ìµœì¢…', header=0)\n",
    "        df_item_raw.columns = df_item_raw.columns.astype(str).str.strip().str.lower()\n",
    "        \n",
    "        df_align = pd.read_excel(FILE_PATH_ALIGN, header=0)\n",
    "        \n",
    "        print(\"âœ… ë°ì´í„° ë¡œë“œ ë° í´ë¦¬ë‹ ì™„ë£Œ.\")\n",
    "        \n",
    "        # XGBoostìš© ë°ì´í„° ì „ì²˜ë¦¬ ë° í†µí•©\n",
    "        df_train_data, df_item_raw_clean = preprocess_for_xgboost(df_user_clean, df_item_raw, df_align)\n",
    "        \n",
    "        # XGBoost í•™ìŠµ ë°ì´í„°ì…‹ ë¶„ë¦¬\n",
    "        # X ë³€ìˆ˜ì— user_idì™€ item_idë¥¼ ë‚¨ê²¨ì„œ í…ŒìŠ¤íŠ¸ì…‹ ì¬êµ¬ì„± ì‹œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "        X = df_train_data.drop(columns=['weight']) # weightë§Œ ì œê±°\n",
    "        y = df_train_data['weight'] # íƒ€ê²Ÿ: ì¬êµ¬ë§¤ ì˜ì‚¬ ì ìˆ˜ (1-5ì )\n",
    "        \n",
    "        # ğŸš¨ í…ŒìŠ¤íŠ¸ì…‹ì„ ë¶„ë¦¬í•  ë•Œ user_idì™€ item_idë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ğŸš¨\n",
    "        X_train_full, X_test_full, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # í›ˆë ¨ì— ì‚¬ìš©í•  í”¼ì²˜(X_train)ì—ì„œëŠ” user_id, item_idë¥¼ ì œê±°í•©ë‹ˆë‹¤.\n",
    "        X_train = X_train_full.drop(columns=['user_id', 'item_id'])\n",
    "        # í…ŒìŠ¤íŠ¸ì— ì‚¬ìš©í•  í”¼ì²˜(X_test)ì—ì„œë„ user_id, item_idë¥¼ ì œê±°í•©ë‹ˆë‹¤.\n",
    "        X_test = X_test_full.drop(columns=['user_id', 'item_id'])\n",
    "\n",
    "        print(f\"\\ní›ˆë ¨ ë°ì´í„° í¬ê¸°: {X_train.shape}, í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {X_test.shape}\")\n",
    "        \n",
    "        # ---------------------------------------------------------------------------------\n",
    "        # 6. XGBoost ëª¨ë¸ í•™ìŠµ (íšŒê·€)\n",
    "        # ---------------------------------------------------------------------------------\n",
    "        # í¬ì†Œí•˜ê³  ë…¸ì´ì¦ˆê°€ ìˆëŠ” ì„¤ë¬¸ ë°ì´í„°ì— ì í•©í•˜ë„ë¡ ì •ê·œí™” ë° í•™ìŠµ ê¹Šì´ ì œì–´\n",
    "        \n",
    "        xgb_model = XGBRegressor(\n",
    "            n_estimators=100,\n",
    "            max_depth=5,            # ê¹Šì´ ì œí•œ (ê³¼ì í•© ë°©ì§€)\n",
    "            learning_rate=0.1,\n",
    "            subsample=0.7,          # ë°ì´í„° ìƒ˜í”Œë§ (ê³¼ì í•© ë°©ì§€)\n",
    "            colsample_bytree=0.7,   # í”¼ì²˜ ìƒ˜í”Œë§\n",
    "            # L2 ì •ê·œí™” \n",
    "            reg_lambda=1.0,         \n",
    "            # L1 ì •ê·œí™”\n",
    "            reg_alpha=0.1,          \n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        print(\"\\nâ–¶ï¸ XGBoost Regressor ëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        print(\"âœ… XGBoost Regressor ëª¨ë¸ í•™ìŠµ ì™„ë£Œ.\")\n",
    "        \n",
    "        # ---------------------------------------------------------------------------------\n",
    "        # 7. ëª¨ë¸ í‰ê°€\n",
    "        # ---------------------------------------------------------------------------------\n",
    "        \n",
    "        # A. íšŒê·€ ì§€í‘œ í‰ê°€ (Regression Metrics)\n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        \n",
    "        # B. ì¶”ì²œ ì§€í‘œ í‰ê°€ (Recommendation Metrics: Precision@K)\n",
    "        # ğŸš¨ X_test_full (user_id, item_id, featuresë¥¼ í¬í•¨)ì— ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë¶™ì—¬ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "        df_test_full = X_test_full.copy()\n",
    "        df_test_full['weight'] = y_test\n",
    "\n",
    "        precision_3 = precision_at_k_recommender(xgb_model, df_test_full, k=3)\n",
    "        precision_5 = precision_at_k_recommender(xgb_model, df_test_full, k=5)\n",
    "\n",
    "        end_time = time.time()\n",
    "        \n",
    "        print(\"\\n-------------------------------------------------\")\n",
    "        print(\"                 XGBoost ëª¨ë¸ í‰ê°€ ê²°ê³¼\")\n",
    "        print(\"-------------------------------------------------\")\n",
    "        print(f\"ì´ ì‹¤í–‰ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ\")\n",
    "        print(\"--- 1. íšŒê·€ ì§€í‘œ (Regression Metrics) ---\")\n",
    "        print(f\"RMSE (Root Mean Squared Error): {rmse:.4f}\")\n",
    "        print(f\"MAE (Mean Absolute Error): {mae:.4f}\")\n",
    "        print(\"\\n--- 2. ì¶”ì²œ ì§€í‘œ (Recommendation Metrics) ---\")\n",
    "        print(f\"Precision@3: {precision_3:.4f}\")\n",
    "        print(f\"Precision@5: {precision_5:.4f}\")\n",
    "        print(\"-------------------------------------------------\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(\"\\nâŒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒìˆ˜ì˜ íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ê±°ë‚˜ Mock Dataë¥¼ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ ìµœì¢… ëª¨ë¸ êµ¬ì¶• ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35323902-11e4-4efa-a564-ef6bebbccbc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9650cd-10ab-4d01-9017-f9c74af6619f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (LightFM Ready)",
   "language": "python",
   "name": "lightfm_python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
