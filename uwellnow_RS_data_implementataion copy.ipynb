{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4fa7c39-1ed2-455b-8a2d-b3faff5cfe83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tabulate in /opt/homebrew/Caskroom/miniconda/base/envs/lightfm_python311/lib/python3.11/site-packages (0.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14dcb0da-7699-4b17-ade8-941279455ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/lightfm_python311/lib/python3.11/site-packages/lightfm/_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightfm.data import Dataset\n",
    "from lightfm import LightFM\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9258762-692b-48f4-b300-1350f601fbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH_MAIN = \"[ìŠ¤íŠ¸ë¡±ë¼ì´í”„]ìµœì¢…_ë°ì´í„°_20250924.xlsx\"\n",
    "\n",
    "\n",
    "def load_and_concatenate_user_data(file_path):\n",
    "    \"\"\"\n",
    "    1ì°¨ì™€ 2ì°¨ ì‹œíŠ¸ë¥¼ ë¡œë“œ, no/no. ì¹¼ëŸ¼ì„ í†µì¼í•œ í›„ ìˆ˜ì§ìœ¼ë¡œ í•©ì¹¨.\n",
    "    \"\"\"\n",
    "\n",
    "    HEADER_ROW_INDEX = 0\n",
    "\n",
    "    # 1ì°¨, 2ì°¨ ì‹œíŠ¸ ë¡œë“œ ë° í—¤ë” ì„¤ì •\n",
    "    df_1st = pd.read_excel(file_path, sheet_name=\"1ì°¨\", header=0)\n",
    "    df_2nd = pd.read_excel(file_path, sheet_name=\"2ì°¨\", header=0)\n",
    "\n",
    "    df_1st.columns = df_1st.columns.astype(str).str.strip()\n",
    "    df_2nd.columns = df_2nd.columns.astype(str).str.strip()\n",
    "\n",
    "    # User ID ì¹¼ëŸ¼ ì´ë¦„ í†µì¼ ('no.' -> 'no')\n",
    "    col_to_rename = {\n",
    "        col: \"no\"\n",
    "        for col in df_2nd.columns\n",
    "        if isinstance(col, str) and col.strip() == \"no.\"\n",
    "    }\n",
    "    if col_to_rename:\n",
    "        df_2nd.rename(columns=col_to_rename, inplace=True)\n",
    "\n",
    "    # ìˆ˜ì§ìœ¼ë¡œ concatenate\n",
    "    df_user_raw = pd.concat([df_1st, df_2nd], ignore_index=True)\n",
    "    df_user_raw.rename(columns={\"no\": \"user_id\"}, inplace=True)\n",
    "\n",
    "    return df_user_raw.set_index(\"user_id\", drop=False)\n",
    "\n",
    "\n",
    "def clean_user_ids(df_user_raw):\n",
    "    \"\"\"\n",
    "    user_id (ì¸ë±ìŠ¤)ì—ì„œ ê²°ì¸¡ì¹˜ ë° ìœ íš¨í•˜ì§€ ì•Šì€ ì”ì—¬ í–‰ì„ ì œê±°í•˜ì—¬ user_idë¥¼ ì •ë¦¬.\n",
    "    \"\"\"\n",
    "    # ë¬¸ìì—´ 'nan'ì„ ì‹¤ì œ ê²°ì¸¡ì¹˜(NaN)ë¡œ ë³€í™˜\n",
    "    df_user_raw.index = df_user_raw.index.to_series().replace(\"nan\", np.nan)\n",
    "\n",
    "    # ì¸ë±ìŠ¤ ê°’ì„ ìˆ«ìí˜•ìœ¼ë¡œ ë³€í™˜ ê°€ëŠ¥í•œì§€ í™•ì¸ (ì˜¤ë¥˜ ì‹œ NaN ì²˜ë¦¬)\n",
    "    valid_user_ids_numeric = pd.to_numeric(df_user_raw.index, errors=\"coerce\")\n",
    "\n",
    "    # 1. user_idê°€ ê²°ì¸¡ì¹˜ê°€ ì•„ë‹ˆê³  (NaNì´ ì•„ë‹ˆê³ )\n",
    "    # 2. user_idê°€ 0ë³´ë‹¤ í° ê°’ì¸ (ìœ íš¨í•œ IDì¸) í–‰ë§Œ ì„ íƒ\n",
    "    valid_indices = df_user_raw.index[\n",
    "        valid_user_ids_numeric.notna() & (valid_user_ids_numeric > 0)\n",
    "    ].unique()\n",
    "\n",
    "    return df_user_raw.loc[valid_indices].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "835e1cba-8fbf-4235-b4aa-3fe035ac1b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì‚¬ìš©ì íŠ¹ì§• ë°ì´í„° concatenate\n",
      "-----------------------------------\n",
      "ì´ ì‚¬ìš©ì ìˆ˜ (ì •ë¦¬ ì „): 1037ëª…\n",
      "ë°ì´í„°í”„ë ˆì„ í¬ê¸° (ì •ë¦¬ ì „): (1038, 147)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # ë°ì´í„° ë¡œë“œ ë° í•©ì¹˜ê¸°\n",
    "    df_user_raw = load_and_concatenate_user_data(FILE_PATH_MAIN)\n",
    "\n",
    "    # user_id ì •ë¦¬ ì „ ìƒíƒœ ì¶œë ¥\n",
    "    print(\"ì‚¬ìš©ì íŠ¹ì§• ë°ì´í„° concatenate\")\n",
    "    print(\"-\" * 35)\n",
    "    print(f\"ì´ ì‚¬ìš©ì ìˆ˜ (ì •ë¦¬ ì „): {df_user_raw.index.nunique()}ëª…\")\n",
    "    print(f\"ë°ì´í„°í”„ë ˆì„ í¬ê¸° (ì •ë¦¬ ì „): {df_user_raw.shape}\")\n",
    "\n",
    "    # 3. user_id ì •ë¦¬\n",
    "    df_user_clean = clean_user_ids(df_user_raw)\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ë°ì´í„° ë¡œë“œ ë˜ëŠ” ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed6cb9d0-214b-4ea3-b13f-392c2d68ff1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•„ì´í…œ íŠ¹ì§• ë°ì´í„° ë¡œë“œ ì™„ë£Œ (df_item_raw)\n",
      "------------------------------\n",
      "\n",
      "--- df_item_raw (ì œí’ˆ íŠ¹ì§• ë°ì´í„°) ê²€ì¦ ---\n",
      "ë¡œë“œëœ Item ì¹¼ëŸ¼ ëª©ë¡: ['product_id', 'brand_name_kor', 'brand_name', 'ingredient_type', 'category', 'sub_category', 'form_factor', 'serving_size', 'serving_unit', 'servings_total', 'calories', 'protein', 'carbs', 'sugars', 'fats', 'Trans Fat', 'Saturated Fat', 'Dietary Fiber', 'ingredients', 'intake_timing', 'product_name', 'sensory_tags', 'functional_tags', 'feature_tags', 'allergens', 'Unnamed: 25']\n"
     ]
    }
   ],
   "source": [
    "# íŒŒì¼ ê²½ë¡œ ì •ì˜\n",
    "FILE_PATH_META = \"ì œí’ˆ ë©”íƒ€ë°ì´í„° ìµœì¢….xlsx\"  # ì œí’ˆ ë©”íƒ€ë°ì´í„° íŒŒì¼\n",
    "\n",
    "# Item ID ì¹¼ëŸ¼ ì´ë¦„ í™•ì • (ì²« ë²ˆì§¸ ì¹¼ëŸ¼ì´ product_idë¼ê³  ê°€ì •)\n",
    "ITEM_ID_COL = \"product_id\"\n",
    "\n",
    "try:\n",
    "    # 1. Item Feature ë°ì´í„° ë¡œë“œ (Header=0 ê°€ì •)\n",
    "    df_item_raw = pd.read_excel(\n",
    "        FILE_PATH_META, sheet_name=\"ì œí’ˆ ë©”íƒ€ë°ì´í„° ìµœì¢…\", header=0\n",
    "    )\n",
    "\n",
    "    # 2. Item ID ì¹¼ëŸ¼ ì´ë¦„ í†µì¼ ë° í™•ì¸\n",
    "    df_item_raw.rename(columns={df_item_raw.columns[0]: ITEM_ID_COL}, inplace=True)\n",
    "\n",
    "    print(\"ì•„ì´í…œ íŠ¹ì§• ë°ì´í„° ë¡œë“œ ì™„ë£Œ (df_item_raw)\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # 3. Item Data í•µì‹¬ ì¹¼ëŸ¼ í™•ì¸\n",
    "    print(\"\\n--- df_item_raw (ì œí’ˆ íŠ¹ì§• ë°ì´í„°) ê²€ì¦ ---\")\n",
    "\n",
    "    # df_item_rawì˜ ëª¨ë“  ì¹¼ëŸ¼ì„ ì¶œë ¥í•˜ì—¬ ì¹¼ëŸ¼ ì´ë¦„ì´ ì œëŒ€ë¡œ ë¡œë“œë˜ì—ˆëŠ”ì§€ ìµœì¢… í™•ì¸\n",
    "    print(f\"ë¡œë“œëœ Item ì¹¼ëŸ¼ ëª©ë¡: {df_item_raw.columns.tolist()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Item Data ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66e8fba2-16ca-4f43-b3ea-803f3be65180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì—‘ì…€ íŒŒì¼ ë¡œë“œ ë° ì •í™•í•œ ID ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„± ì™„ë£Œ.\n",
      "**ì›ë³¸ ë°ì´í„°í”„ë ˆì„ (df_align)ì€ 7ê°œì˜ ì»¬ëŸ¼ìœ¼ë¡œ ë©”ëª¨ë¦¬ì— ìœ ì§€ë©ë‹ˆë‹¤.**\n"
     ]
    }
   ],
   "source": [
    "# 2. ì—‘ì…€ íŒŒì¼ì„ ì½ì–´ ë°ì´í„°í”„ë ˆì„ ì „ì²´ë¥¼ ë¡œë“œ (ëª¨ë“  ì»¬ëŸ¼ ìœ ì§€)\n",
    "FILE_PATH_ALIGN = \"uwellnow_product_align.xlsx\"\n",
    "try:\n",
    "    df_align = pd.read_excel(FILE_PATH_ALIGN)\n",
    "\n",
    "    # 3. ë§¤í•‘ í‚¤ ìƒì„± ë° ë”•ì…”ë„ˆë¦¬ êµ¬ì¶•\n",
    "    # ë‘ ì»¬ëŸ¼ì„ ì¡°í•©í•˜ì—¬ ìƒˆë¡œìš´ í‚¤ (Key)ë¥¼ ë§Œë“­ë‹ˆë‹¤: \"PRODUCT_FLAVOR\"\n",
    "    df_align[\"MAPPING_KEY\"] = (\n",
    "        df_align[\"product\"].astype(str).str.strip().str.upper()\n",
    "        + \"_\"\n",
    "        + df_align[\"flavor\"].astype(str).str.strip().str.upper()\n",
    "    )\n",
    "\n",
    "    # 4. ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„±: {PRODUCT_FLAVOR: product_id}\n",
    "    ITEM_FULL_ID_MAP = pd.Series(\n",
    "        df_align[\"product_id\"].astype(str).str.strip().str.upper().values,\n",
    "        index=df_align[\"MAPPING_KEY\"],\n",
    "    ).to_dict()\n",
    "\n",
    "    # ë”•ì…”ë„ˆë¦¬ì—ì„œ NaN (ê²°ì¸¡ì¹˜) ê´€ë ¨ í•­ëª©ì€ ì œê±° (ì„ íƒ ì‚¬í•­)\n",
    "    if \"NAN_NAN\" in ITEM_FULL_ID_MAP:\n",
    "        del ITEM_FULL_ID_MAP[\"NAN_NAN\"]\n",
    "\n",
    "    print(\"âœ… ì—‘ì…€ íŒŒì¼ ë¡œë“œ ë° ì •í™•í•œ ID ë§¤í•‘ ë”•ì…”ë„ˆë¦¬ ìƒì„± ì™„ë£Œ.\")\n",
    "    print(\n",
    "        f\"**ì›ë³¸ ë°ì´í„°í”„ë ˆì„ (df_align)ì€ {df_align.shape[1]}ê°œì˜ ì»¬ëŸ¼ìœ¼ë¡œ ë©”ëª¨ë¦¬ì— ìœ ì§€ë©ë‹ˆë‹¤.**\"\n",
    "    )\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        f\"âŒ ì˜¤ë¥˜: íŒŒì¼ ê²½ë¡œ '{FILE_PATH_ALIGN}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\"\n",
    "    )\n",
    "    ITEM_FULL_ID_MAP = {}\n",
    "    df_align = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4603628-d843-4e77-aa4a-dcdc0d9aea4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ê°œì„ ëœ í•¨ìˆ˜ ì‚¬ìš© ì˜ˆì‹œ (AC, AD ë¶„ë¦¬ëœ ê²½ìš°) ---\n",
      "('ê²Œí† ë ˆì´ íŒŒìš°ë”', 'ê²Œí† ë ˆì´ë§›') -> ['GATORADE_POWDER_LEMONLIME']\n",
      "('ì˜µí‹°ë©ˆ ì›¨ì´', 'ì´ˆì½œë¦¿, ë°”ë‹ë¼') -> ['OPTIMUM_GSWHEY_CHOCOLATE', 'OPTIMUM_GSWHEY_VANILLA']\n",
      "('ìƒˆë¡œìš´ ë‹¨ë°±ì§ˆ', 'ë°”ë‚˜ë‚˜ë§›,í‚¤ìœ„ë§›') -> ['ìƒˆë¡œìš´ë‹¨ë°±ì§ˆë°”ë‚˜ë‚˜ë§›,í‚¤ìœ„ë§›']\n"
     ]
    }
   ],
   "source": [
    "# 2) ì‚¬ìš©ì ì‘ë‹µ ì œí’ˆëª…ì„ ì •í™•í•œ product_idë¡œ ë§¤í•‘í•˜ëŠ” í•¨ìˆ˜ (AC ì»¬ëŸ¼ + AD ì»¬ëŸ¼ ì‚¬ìš©)\n",
    "def normalize_interaction_id(product_name_ac, flavor_ad, mapping_dict=ITEM_FULL_ID_MAP):\n",
    "\n",
    "    if not product_name_ac or not flavor_ad or not mapping_dict:\n",
    "        # ìœ íš¨í•˜ì§€ ì•Šì€ ì…ë ¥ì˜ ê²½ìš° None ë°˜í™˜\n",
    "        return [None]\n",
    "\n",
    "    product_clean = str(product_name_ac).strip().upper()\n",
    "    flavor_input = str(flavor_ad).strip().upper()\n",
    "\n",
    "    # 1. ì½¤ë§ˆ(,)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë§›ì„ ë¶„ë¦¬í•˜ê³ , ê³µë°±ì„ ì œê±°\n",
    "    # 'ë°”ë‹ë¼, ì´ˆì½œë¦¿' -> ['ë°”ë‹ë¼', 'ì´ˆì½œë¦¿']\n",
    "    flavor_list = [f.strip() for f in flavor_input.split(\",\") if f.strip()]\n",
    "\n",
    "    found_product_ids = []\n",
    "\n",
    "    # 2. ë¶„ë¦¬ëœ ê° ë§›ì— ëŒ€í•´ ë”•ì…”ë„ˆë¦¬ ê²€ìƒ‰ ì‹œë„\n",
    "    for flavor_clean in flavor_list:\n",
    "        # ë”•ì…”ë„ˆë¦¬ ê²€ìƒ‰ì— ì‚¬ìš©í•  ìµœì¢… í‚¤ ì¡°í•©: \"PRODUCT_FLAVOR\"\n",
    "        search_key = f\"{product_clean}_{flavor_clean}\"\n",
    "\n",
    "        # ë”•ì…”ë„ˆë¦¬ì—ì„œ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸\n",
    "        if search_key in mapping_dict:\n",
    "            found_product_ids.append(mapping_dict[search_key])\n",
    "\n",
    "    # 3. ìœ íš¨í•œ IDê°€ ë°œê²¬ëœ ê²½ìš°\n",
    "    if found_product_ids:\n",
    "        # ì¤‘ë³µ ì œê±° í›„ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\n",
    "        return list(set(found_product_ids))\n",
    "\n",
    "    # 4. ë§¤í•‘ë˜ì§€ ì•Šì€ ê²½ìš°: ì•ˆì „ ì¥ì¹˜ (ì œí’ˆëª…+ì›ë˜ ë§› ë¬¸ìì—´)ë¥¼ ë‹¨ì¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜\n",
    "    # ì´ ë¶€ë¶„ì€ LightFM item IDë¡œ ì‚¬ìš©í•˜ê¸°ì— ì í•©í•œ ë¬¸ìì—´ë¡œ ê°€ê³µë©ë‹ˆë‹¤.\n",
    "    combined_name = f\"{product_clean}{flavor_input}\"\n",
    "    fallback_id = (\n",
    "        combined_name.replace(\" \", \"\")\n",
    "        .replace(\"-\", \"\")\n",
    "        .replace(\".\", \"\")\n",
    "        .replace(\"(\", \"\")\n",
    "        .replace(\")\", \"\")\n",
    "    )\n",
    "    return [fallback_id]\n",
    "\n",
    "\n",
    "# --- ì‚¬ìš© ì˜ˆì‹œ ---\n",
    "print(\"\\n--- ê°œì„ ëœ í•¨ìˆ˜ ì‚¬ìš© ì˜ˆì‹œ (AC, AD ë¶„ë¦¬ëœ ê²½ìš°) ---\")\n",
    "\n",
    "# ì˜ˆì‹œ 1: ë‹¨ì¼ ë§› (ê¸°ì¡´ê³¼ ë™ì¼)\n",
    "product_in1 = \"ê²Œí† ë ˆì´ íŒŒìš°ë”\"\n",
    "flavor_in1 = \"ê²Œí† ë ˆì´ë§›\"\n",
    "result1 = normalize_interaction_id(\n",
    "    product_in1,\n",
    "    flavor_in1,\n",
    "    mapping_dict={\"ê²Œí† ë ˆì´ íŒŒìš°ë”_ê²Œí† ë ˆì´ë§›\": \"GATORADE_POWDER_LEMONLIME\"},\n",
    ")\n",
    "print(f\"('{product_in1}', '{flavor_in1}') -> {result1}\")\n",
    "# ì˜ˆìƒ ê²°ê³¼: ['GATORADE_POWDER_LEMONLIME']\n",
    "\n",
    "# ì˜ˆì‹œ 2: ë‹¤ì¤‘ ë§› í¬í•¨ (ì½¤ë§ˆë¡œ êµ¬ë¶„)\n",
    "product_in2 = \"ì˜µí‹°ë©ˆ ì›¨ì´\"\n",
    "flavor_in2 = \"ì´ˆì½œë¦¿, ë°”ë‹ë¼\"\n",
    "test_map = {\n",
    "    \"ì˜µí‹°ë©ˆ ì›¨ì´_ì´ˆì½œë¦¿\": \"OPTIMUM_GSWHEY_CHOCOLATE\",\n",
    "    \"ì˜µí‹°ë©ˆ ì›¨ì´_ë°”ë‹ë¼\": \"OPTIMUM_GSWHEY_VANILLA\",\n",
    "    \"ì˜µí‹°ë©ˆ ì›¨ì´_ë”¸ê¸°\": \"OPTIMUM_GSWHEY_STRAWBERRY\",\n",
    "}\n",
    "result2 = normalize_interaction_id(product_in2, flavor_in2, mapping_dict=test_map)\n",
    "print(f\"('{product_in2}', '{flavor_in2}') -> {result2}\")\n",
    "# ì˜ˆìƒ ê²°ê³¼: ['OPTIMUM_GSWHEY_CHOCOLATE', 'OPTIMUM_GSWHEY_VANILLA']\n",
    "\n",
    "# ì˜ˆì‹œ 3: ë§¤í•‘ë˜ì§€ ì•Šì€ ê²½ìš° (ì•ˆì „ì¥ì¹˜ ë°œë™)\n",
    "product_in3 = \"ìƒˆë¡œìš´ ë‹¨ë°±ì§ˆ\"\n",
    "flavor_in3 = \"ë°”ë‚˜ë‚˜ë§›,í‚¤ìœ„ë§›\"\n",
    "result3 = normalize_interaction_id(product_in3, flavor_in3, mapping_dict=test_map)\n",
    "print(f\"('{product_in3}', '{flavor_in3}') -> {result3}\")\n",
    "# ì˜ˆìƒ ê²°ê³¼: ['ìƒˆë¡œìš´ë‹¨ë°±ì§ˆë°”ë‚˜ë‚˜ë§›,í‚¤ìœ„ë§›']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cd1f37d-f5b6-4288-a24f-486ab970da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LightFM ëª¨ë¸ë§ ë³€ìˆ˜ ì •ì˜ ---\n",
    "\n",
    "# ì‚¬ìš©ì íŠ¹ì§• (User Feature, Multi-Hot Encoding ëŒ€ìƒ)\n",
    "OHE_USER_COLS = [\n",
    "    # Feature Group 1 (ê¸°ë³¸ ì •ë³´ ë° í™œë™ íŒ¨í„´)\n",
    "    \"3) ì„±ë³„\",\n",
    "    \"8) ìš´ë™ í™œë™ ê¸°ê°„\",\n",
    "    \"7) í”„ë¡œí‹´, í”„ë¦¬ì›Œí¬ì•„ì›ƒ, ì „í•´ì§ˆ ìŒë£Œ, ê²Œì´ë„ˆ ë“± í—¬ìŠ¤ ë³´ì¶©ì œ 2ì¢… ì´ìƒì„ ì„­ì·¨í•´ ë³´ì‹  ê²½í—˜ì´ ìˆìœ¼ì‹ ê°€ìš”?\",\n",
    "    \"9) ì£¼ì— ëª‡ íšŒ ì •ë„ ìš´ë™ì„ ì§„í–‰í•˜ì‹œë‚˜ìš”?(íƒ1)\",\n",
    "    \"10) ì•ŒëŸ¬ì§€ ë˜ëŠ” ë¯¼ê°ì„±ë¶„(ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "    \"11) í‰ì†Œ ì±™ê¸°ëŠ” ë¼ë‹ˆëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?(ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "    \"12) ì‹ì‚¬ ê¸°ì¤€ìœ¼ë¡œ ìš´ë™ ì‹œê°„ì€ ì–¸ì œì¸ê°€ìš”?(íƒ 1)\",\n",
    "    \"12-1) ì¼ê³¼(ìˆ˜ì—…,ì—…ë¬´,ì¼ ë“±) ê¸°ì¤€ìœ¼ë¡œ ìš´ë™ ì‹œê°„ì€ ì–¸ì œì¸ê°€ìš”?(íƒ 1)\",\n",
    "    \"12-2) ìš´ë™ì„ ì œì™¸í•œ ì¼ê³¼ ì¤‘ í™œë™ì€ ì–´ëŠ ì •ë„ë¡œ í™œë°œí•œê°€ìš”?(íƒ 1)\",\n",
    "    \"12-3) ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ìš´ë™ ì‹œì‘ ì‹œê°„ì´ ì–¸ì œì¸ê°€ìš”?(íƒ 1)\",\n",
    "    # Feature Group 2 (ì˜í–¥ ìš”ì¸ ë° ê³ ë ¤ í•­ëª©) - ëª¨ë“  ë³´ì¶©ì œ ê´€ë ¨ ê³ ë ¤ í•­ëª© í¬í•¨\n",
    "    # \"13-3) í”„ë¡œí‹´ì˜ ì˜ì–‘ì„±ë¶„ì„ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "    # \"13-4) í”„ë¡œí‹´ì˜ íš¨ê³¼ì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "    \"13-5) ë¸Œëœë“œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "    # \"13-6) ìœ ëª…ì¸(ì„ ìˆ˜ ë˜ëŠ” ì „ë¬¸ê°€)ì˜ ì‚¬ìš© ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "    # \"13-7) ì§€ì¸ì˜ ì‚¬ìš©ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "    \"14-3) í”„ë¦¬ì›Œí¬ì•„ì›ƒì˜ ì˜ì–‘ì„±ë¶„ì„ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "    \"14-4) í”„ë¦¬ì›Œí¬ì•„ì›ƒì˜ íš¨ê³¼ì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "    \"14-5) ë¸Œëœë“œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "    # \"14-6) ìœ ëª…ì¸(ì„ ìˆ˜ ë˜ëŠ” ì „ë¬¸ê°€)ì˜ ì‚¬ìš© ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "    # \"14-7) ì§€ì¸ì˜ ì‚¬ìš©ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "    \"15-3) ì „í•´ì§ˆ ìŒë£Œì˜ ì˜ì–‘ì„±ë¶„ì„ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "    \"15-4) ì „í•´ì§ˆ ìŒë£Œì˜ íš¨ê³¼ì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "    \"15-5) ë¸Œëœë“œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "    # \"15-6) ìœ ëª…ì¸(ì„ ìˆ˜ ë˜ëŠ” ì „ë¬¸ê°€)ì˜ ì‚¬ìš© ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "    # \"15-7) ì§€ì¸ì˜ ì‚¬ìš©ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "    \"16-3) ê²Œì´ë„ˆì˜ ì˜ì–‘ì„±ë¶„ì„ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "    \"16-4) ê²Œì´ë„ˆì˜ íš¨ê³¼ì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "    \"16-5) ë¸Œëœë“œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "    # \"16-6) ìœ ëª…ì¸(ì„ ìˆ˜ ë˜ëŠ” ì „ë¬¸ê°€)ì˜ ì‚¬ìš© ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "    # \"16-7) ì§€ì¸ì˜ ì‚¬ìš©ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "    # \"17-3) í•´ë‹¹ ë³´ì¶©ì œì˜ ì˜ì–‘ì„±ë¶„ì„ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "    # \"17-4) í•´ë‹¹ ë³´ì¶©ì œì˜ íš¨ê³¼ì—ì„œ ê³ ë ¤í•œ ì ì€ ë¬´ì—‡ì¸ê°€ìš”?\",\n",
    "    # \"17-5) ë¸Œëœë“œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "    # \"17-6) ìœ ëª…ì¸(ì„ ìˆ˜ ë˜ëŠ” ì „ë¬¸ê°€)ì˜ ì‚¬ìš© ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "    # \"17-7) ì§€ì¸ì˜ ì‚¬ìš©ì—¬ë¶€ ë˜ëŠ” ì¶”ì²œì—ì„œ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "]\n",
    "\n",
    "# B. ì•„ì´í…œ íŠ¹ì§• (Item Feature, Multi-Hot Encoding ëŒ€ìƒ)\n",
    "OHE_ITEM_COLS = [\"ingredient_type\", \"category\", \"flavor\", \"sensory_tags\"]\n",
    "\n",
    "# C. ìƒí˜¸ì‘ìš© (Interaction, One-Hot Encoding ëŒ€ìƒ)\n",
    "# 'ê°€ì¥ ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆ'ì„ Item IDë¡œ ì‚¬ìš©í•˜ê³ , 'ì¬êµ¬ë§¤ ì˜ì‚¬'ë¥¼ ê°€ì¤‘ì¹˜ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "INTERACTION_WEIGHT_COLS = [\n",
    "    (\n",
    "        \"13-9) [í”„ë¡œí‹´] ì„ íƒí•˜ì‹  ì œí’ˆì¤‘ì— ê°€ì¥ ì¢…í•©ì ìœ¼ë¡œ ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆ 1ê°œë§Œ ì„ íƒí•´ì£¼ì„¸ìš” (íƒ 1)\",\n",
    "        \"13-10) ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆì´ ì–´ë–¤ ë§›ì¸ê°€ìš”? (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "        \"13-17) í•´ë‹¹ í”„ë¡œí‹´ì— ëŒ€í•œ ì¬êµ¬ë§¤ ì˜ì‚¬ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”?\",\n",
    "    ),\n",
    "    (\n",
    "        \"14-9) [í”„ë¦¬ì›Œí¬ì•„ì›ƒ] ì„ íƒí•˜ì‹  ì œí’ˆì¤‘ì— ê°€ì¥ ì¢…í•©ì ìœ¼ë¡œ ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆ 1ê°œë§Œ ì„ íƒí•´ì£¼ì„¸ìš” (íƒ 1)\",\n",
    "        \"14-10) ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆì´ ì–´ë–¤ ë§›ì¸ê°€ìš”? (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "        \"14-17) í•´ë‹¹ í”„ë¦¬ì›Œí¬ì•„ì›ƒì— ëŒ€í•œ ì¬êµ¬ë§¤ ì˜ì‚¬ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”?\",\n",
    "    ),\n",
    "    (\n",
    "        \"15-9) [ì „í•´ì§ˆ ìŒë£Œ(BCAA, ì´ì˜¨ìŒë£Œ)] ì„ íƒí•˜ì‹  ì œí’ˆì¤‘ì— ê°€ì¥ ì¢…í•©ì ìœ¼ë¡œ ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆ 1ê°œë§Œ ì„ íƒí•´ì£¼ì„¸ìš” (íƒ 1)\",\n",
    "        \"15-10) ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆì´ ì–´ë–¤ ë§›ì¸ê°€ìš”? (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "        \"15-17) í•´ë‹¹ ì „í•´ì§ˆ ìŒë£Œì— ëŒ€í•œ ì¬êµ¬ë§¤ ì˜ì‚¬ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”?\",\n",
    "    ),\n",
    "    (\n",
    "        \"16-9) [ê²Œì´ë„ˆ] ì„ íƒí•˜ì‹  ì œí’ˆì¤‘ì— ê°€ì¥ ì¢…í•©ì ìœ¼ë¡œ ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆ 1ê°œë§Œ ì„ íƒí•´ì£¼ì„¸ìš” (íƒ 1)\",\n",
    "        \"16-10) ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆì´ ì–´ë–¤ ë§›ì¸ê°€ìš”? (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "        \"16-17) í•´ë‹¹ ê²Œì´ë„ˆì— ëŒ€í•œ ì¬êµ¬ë§¤ ì˜ì‚¬ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”?\",\n",
    "    ),\n",
    "    (\n",
    "        \"17-9) ì¢…í•©ì ìœ¼ë¡œ ê°€ì¥ ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆ 1ê°œë¥¼ ì‘ì„±í•´ ì£¼ì„¸ìš”\",\n",
    "        \"17-10) ë§ˆìŒì— ë“¤ì—ˆë˜ ì œí’ˆì´ ì–´ë–¤ ë§›ì¸ê°€ìš”?\",\n",
    "        \"17-17) í•´ë‹¹ ì œí’ˆì— ëŒ€í•œ ì¬êµ¬ë§¤ ì˜ì‚¬ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”?\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43e96d12-3a98-47b2-8043-16f16d45cc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: OHE_ITEM_COLS ëª©ë¡: ['ingredient_type', 'category', 'flavor', 'sensory_tags']\n",
      "------------------------------\n",
      "DEBUG: Item Features ì´ ê°œìˆ˜ (ìƒì„±ëœ í”¼ì²˜ ì¢…ë¥˜): 136\n",
      "DEBUG: Item Features í–‰ ê°œìˆ˜ (ìƒí˜¸ì‘ìš©): 685\n",
      "âœ… Item Feature (ê°€ì¤‘ì¹˜ í¬í•¨) ì „ì²˜ë¦¬ ì™„ë£Œ.\n",
      "DEBUG: df_interactions ì´ˆê¸° ìƒì„± í›„ ì»¬ëŸ¼: ['user_id', 'weight', 'item_id']\n",
      "DEBUG: df_interactions ì´ˆê¸° ìƒì„± í›„ shape: (1008, 3)\n",
      "ğŸ“Š ì¸ê¸°í¸í–¥ íŒ¨ë„í‹° ì ìš© ì „/í›„ ë¹„êµ:\n",
      "  - ìƒìœ„ ì œí’ˆ ê°€ì¤‘ì¹˜ ê°ì†Œìœ¨: ~70% (POP_PENALTY_ALPHA=10.0)\n",
      "âœ… Interaction Matrix ì†ŒìŠ¤ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ. (ì¸ê¸°í¸í–¥ ì™„í™” ì ìš©)\n",
      "âœ… User Feature Matrix ì†ŒìŠ¤ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ. (ë­í‚¹ ê°€ì¤‘ì¹˜ í†µí•©)\n",
      "\n",
      "--- 8.5ë‹¨ê³„: User Feature ê¸°ë°˜ k-NN ë°ì´í„° ë³´ê°• ì‹œì‘ ---\n",
      "âœ… ë³´ê°•ëœ ìƒí˜¸ì‘ìš© 5109ê°œ ì¶”ê°€. ìµœì¢… Interaction í–‰ ê°œìˆ˜: 4528\n",
      "DEBUG: k-NN ë³´ê°• í›„ df_interactions ì»¬ëŸ¼: ['user_id', 'weight', 'item_id']\n",
      "DEBUG: k-NN ë³´ê°• í›„ df_interactions shape: (4528, 3)\n",
      "\n",
      "DEBUG: df_interactions ì»¬ëŸ¼: ['user_id', 'weight', 'item_id']\n",
      "DEBUG: df_interactions shape: (4528, 3)\n",
      "DEBUG: df_item_timing ì»¬ëŸ¼: ['product_id', 'timing_category']\n",
      "DEBUG: ITEM_ID_COL ê°’: product_id\n",
      "DEBUG: df_interactions_with_timing ì»¬ëŸ¼: ['user_id', 'weight', 'item_id', 'timing_category']\n",
      "DEBUG: df_interactions_with_timing shape: (4722, 4)\n",
      "\n",
      "============================================================\n",
      "íƒ€ì´ë°ë³„ ëª¨ë¸ í•™ìŠµ: Pre\n",
      "============================================================\n",
      "DEBUG: Pre - ì‚¬ìš©ì ìˆ˜: 338, ì•„ì´í…œ ìˆ˜: 29\n",
      "DEBUG: Pre - Item features ìˆ˜: 19\n",
      "âœ… Pre ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\n",
      "  Train: (338, 29) / Non-zero: 1195\n",
      "  Test: (338, 29) / Non-zero: 298\n",
      "\n",
      "============================================================\n",
      "íƒ€ì´ë°ë³„ ëª¨ë¸ í•™ìŠµ: Intra\n",
      "============================================================\n",
      "DEBUG: Intra - ì‚¬ìš©ì ìˆ˜: 637, ì•„ì´í…œ ìˆ˜: 14\n",
      "DEBUG: Intra - Item features ìˆ˜: 15\n",
      "âœ… Intra ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\n",
      "  Train: (637, 14) / Non-zero: 944\n",
      "  Test: (637, 14) / Non-zero: 235\n",
      "\n",
      "============================================================\n",
      "íƒ€ì´ë°ë³„ ëª¨ë¸ í•™ìŠµ: Post\n",
      "============================================================\n",
      "DEBUG: Post - ì‚¬ìš©ì ìˆ˜: 925, ì•„ì´í…œ ìˆ˜: 12\n",
      "DEBUG: Post - Item features ìˆ˜: 12\n",
      "âœ… Post ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\n",
      "  Train: (925, 12) / Non-zero: 1640\n",
      "  Test: (925, 12) / Non-zero: 410\n",
      "\n",
      "\n",
      "âœ… íƒ€ì´ë°ë³„ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\n"
     ]
    }
   ],
   "source": [
    "# df_user_cleanì˜ ì¸ë±ìŠ¤ ì´ë¦„ ì •ë¦¬ (ì˜¤ë¥˜ í•´ê²°)\n",
    "if df_user_clean.index.name == \"user_id\":\n",
    "    df_user_clean.index.name = None\n",
    "\n",
    "# k-NN ë¡œì§ì—ì„œ ì‚¬ìš©í•  ë³€ìˆ˜ ì´ˆê¸°í™” (try ë¸”ë¡ ë°–ì—ì„œ ì •ì˜)\n",
    "user_features_matrix = None\n",
    "dataset = None\n",
    "\n",
    "try:\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # 5ë‹¨ê³„: Item Feature Matrix ì†ŒìŠ¤ ë°ì´í„° êµ¬ì¶• (ê°€ì¤‘ì¹˜ ì ìš©)\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    print(f\"DEBUG: OHE_ITEM_COLS ëª©ë¡: {OHE_ITEM_COLS}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # ... (df_item_raw ì •ë¦¬ ë° selected_ohe_cols ì •ì˜ ë¡œì§ì€ ë™ì¼)\n",
    "    df_item_raw.dropna(subset=[ITEM_ID_COL], inplace=True)\n",
    "    selected_ohe_cols = [col for col in OHE_ITEM_COLS if col in df_item_raw.columns]\n",
    "\n",
    "    df_item_features = df_item_raw.melt(\n",
    "        id_vars=[ITEM_ID_COL], value_vars=selected_ohe_cols\n",
    "    )\n",
    "\n",
    "    df_item_features[\"value\"] = df_item_features[\"value\"].fillna(\"NONE\")\n",
    "    df_item_features[\"feature\"] = (\n",
    "        df_item_features[\"variable\"].astype(str)\n",
    "        + \"_\"\n",
    "        + df_item_features[\"value\"].astype(str).str.strip()\n",
    "    )\n",
    "\n",
    "    # Item Feature ê°€ì¤‘ì¹˜ ë¶€ì—¬ ë¡œì§ (ê· í˜• ì¡°ì •)\n",
    "    df_item_features[\"weight\"] = 1.3  # ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì•½ê°„ ì¦ê°€\n",
    "    df_item_features.loc[df_item_features[\"variable\"] == \"category\", \"weight\"] = (\n",
    "        2  # ì¹´í…Œê³ ë¦¬ 2ë°°\n",
    "    )\n",
    "    df_item_features.loc[\n",
    "        df_item_features[\"variable\"] == \"ingredient_type\", \"weight\"\n",
    "    ] = 2  # ì›ì¬ë£Œ íƒ€ì… 2ë°°\n",
    "\n",
    "    # ğŸŒŸ ìµœì¢… ì •ë¦¬: 'weight' ì»¬ëŸ¼ì„ í¬í•¨í•˜ë„ë¡ ìˆ˜ì • ğŸŒŸ\n",
    "    df_item_features = df_item_features[\n",
    "        [ITEM_ID_COL, \"feature\", \"weight\"]\n",
    "    ].drop_duplicates(subset=[ITEM_ID_COL, \"feature\"], keep=\"first\")\n",
    "\n",
    "    print(\n",
    "        f\"DEBUG: Item Features ì´ ê°œìˆ˜ (ìƒì„±ëœ í”¼ì²˜ ì¢…ë¥˜): {len(df_item_features['feature'].unique())}\"\n",
    "    )\n",
    "    print(f\"DEBUG: Item Features í–‰ ê°œìˆ˜ (ìƒí˜¸ì‘ìš©): {len(df_item_features)}\")\n",
    "    print(\"âœ… Item Feature (ê°€ì¤‘ì¹˜ í¬í•¨) ì „ì²˜ë¦¬ ì™„ë£Œ.\")\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # 6ë‹¨ê³„: Interaction Matrix ì†ŒìŠ¤ ë°ì´í„° êµ¬ì¶•\n",
    "    # ---------------------------------------------------------------------------------\n",
    "\n",
    "    RANKING_COLS_MAP = [\n",
    "        # (ìˆœìœ„ ì»¬ëŸ¼, ê°€ì¤‘ì¹˜ ì»¬ëŸ¼, ì œí’ˆêµ° íƒœê·¸, ì²« ë²ˆì§¸ í•­ëª© ì»¬ëŸ¼)\n",
    "        (\n",
    "            \"13-2) í”„ë¡œí‹´ ì„ íƒ ì‹œ ê°€ì¥ ì˜í–¥ì„ ë§ì´ ë°›ì€ ìš”ì†Œë“¤ì˜ ìˆœìœ„ë¥¼ ë§¤ê²¨ì£¼ì„¸ìš”\",\n",
    "            \"13-17) í•´ë‹¹ í”„ë¡œí‹´ì— ëŒ€í•œ ì¬êµ¬ë§¤ ì˜ì‚¬ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”? Â \",\n",
    "            \"PROTEIN\",\n",
    "            \"13-3) í”„ë¡œí‹´ì˜ ì˜ì–‘ì„±ë¶„ì„ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "        ),\n",
    "        (\n",
    "            \"14-2) í”„ë¦¬ì›Œí¬ì•„ì›ƒ ì„ íƒ ì‹œ ê°€ì¥ ì˜í–¥ì„ ë§ì´ ë°›ì€ ìš”ì†Œë“¤ì˜ ìˆœìœ„ë¥¼ ë§¤ê²¨ì£¼ì„¸ìš”\",\n",
    "            \"14-17) í•´ë‹¹ í”„ë¦¬ì›Œí¬ì•„ì›ƒì— ëŒ€í•œ ì¬êµ¬ë§¤ ì˜ì‚¬ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”? Â \",\n",
    "            \"PREWORKOUT\",\n",
    "            \"14-3) í”„ë¦¬ì›Œí¬ì•„ì›ƒì˜ ì˜ì–‘ì„±ë¶„ì„ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "        ),\n",
    "        (\n",
    "            \"15-2) ì „í•´ì§ˆ ìŒë£Œ(BCAA, ì´ì˜¨ìŒë£Œ)ì„ íƒ ì‹œ ê°€ì¥ ì˜í–¥ì„ ë§ì´ ë°›ì€ ìš”ì†Œë“¤ì˜ ìˆœìœ„ë¥¼ ë§¤ê²¨ì£¼ì„¸ìš”\",\n",
    "            \"15-17) í•´ë‹¹ ì „í•´ì§ˆ ìŒë£Œì— ëŒ€í•œ ì¬êµ¬ë§¤ ì˜ì‚¬ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”? Â \",\n",
    "            \"ELECTROLYTE\",\n",
    "            \"15-3) ì „í•´ì§ˆ ìŒë£Œì˜ ì˜ì–‘ì„±ë¶„ì„ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "        ),\n",
    "        (\n",
    "            \"16-2) ê²Œì´ë„ˆ ì„ íƒ ì‹œ ê°€ì¥ ì˜í–¥ì„ ë§ì´ ë°›ì€ ìš”ì†Œë“¤ì˜ ìˆœìœ„ë¥¼ ë§¤ê²¨ì£¼ì„¸ìš”\",\n",
    "            \"16-17) í•´ë‹¹ ê²Œì´ë„ˆì— ëŒ€í•œ ì¬êµ¬ë§¤ ì˜ì‚¬ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”? Â \",\n",
    "            \"GAINER\",\n",
    "            \"16-3) ê²Œì´ë„ˆì˜ ì˜ì–‘ì„±ë¶„ì„ ê³ ë ¤í•œ ì„¸ë¶€ í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš” (ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "        ),\n",
    "    ]\n",
    "    MAX_RANK_COUNT = 7\n",
    "\n",
    "    # ìˆœìœ„ì˜ ì—­ìˆœìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ëŠ” í•¨ìˆ˜\n",
    "    def rank_to_weight(rank_value, max_rank=MAX_RANK_COUNT):\n",
    "        rank_value = pd.to_numeric(rank_value, errors=\"coerce\")\n",
    "        if pd.isna(rank_value):\n",
    "            return np.nan\n",
    "        # 1ìˆœìœ„ -> 7, 7ìˆœìœ„ -> 1\n",
    "        return (max_rank + 1) - rank_value\n",
    "\n",
    "    df_interactions_list = []\n",
    "\n",
    "    # --- 1. ì œí’ˆëª… + ë§› ê¸°ë°˜ ìƒí˜¸ì‘ìš© (explode ì ìš©) ---\n",
    "    for item_col, flavor_col, weight_col in INTERACTION_WEIGHT_COLS:\n",
    "        temp_df = df_user_clean[[\"user_id\", item_col, flavor_col, weight_col]].copy()\n",
    "\n",
    "        temp_df.rename(\n",
    "            columns={item_col: \"product\", flavor_col: \"flavor\", weight_col: \"weight\"},\n",
    "            inplace=True,\n",
    "        )\n",
    "        temp_df.dropna(subset=[\"product\", \"flavor\"], inplace=True)\n",
    "        temp_df[\"item_id_list\"] = temp_df.apply(\n",
    "            lambda row: normalize_interaction_id(row[\"product\"], row[\"flavor\"]), axis=1\n",
    "        )\n",
    "        temp_df = temp_df.explode(\"item_id_list\")\n",
    "        temp_df.rename(columns={\"item_id_list\": \"item_id\"}, inplace=True)\n",
    "        temp_df.drop(columns=[\"product\", \"flavor\"], inplace=True)\n",
    "\n",
    "        # ìƒí˜¸ì‘ìš© ê°€ì¤‘ì¹˜ ê³¼ë„ ì¦í­ ì™„í™” (ì¸ê¸°í¸í–¥ ì™„í™”)\n",
    "        temp_df[\"weight\"] = pd.to_numeric(temp_df[\"weight\"], errors=\"coerce\") * 3\n",
    "        temp_df[\"weight\"] = temp_df[\"weight\"].clip(lower=1, upper=10)\n",
    "\n",
    "        df_interactions_list.append(temp_df)\n",
    "\n",
    "    # --- 2. ìµœì¢… Interaction ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì •ë¦¬ ---\n",
    "    df_interactions = pd.concat(df_interactions_list, ignore_index=True)\n",
    "    df_interactions[\"weight\"] = pd.to_numeric(\n",
    "        df_interactions[\"weight\"], errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "    valid_items = df_item_raw[ITEM_ID_COL].unique()\n",
    "    df_interactions = df_interactions[df_interactions[\"item_id\"].isin(valid_items)]\n",
    "\n",
    "    df_interactions.dropna(subset=[\"item_id\", \"weight\"], inplace=True)\n",
    "\n",
    "    print(\n",
    "        f\"DEBUG: df_interactions ì´ˆê¸° ìƒì„± í›„ ì»¬ëŸ¼: {df_interactions.columns.tolist()}\"\n",
    "    )\n",
    "    print(f\"DEBUG: df_interactions ì´ˆê¸° ìƒì„± í›„ shape: {df_interactions.shape}\")\n",
    "\n",
    "    # ğŸ”¥\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    # í¸í–¥ ê°•ë ¥ ì™„í™”: ì¸ê¸°ë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì¡°ì • (ë°ì´í„° ì¤‘ì‹¬ìœ¼ë¡œ ì¡°ì •)\n",
    "    POP_PENALTY_ALPHA = 10.0  # ì¸ê¸°í¸í–¥ íŒ¨ë„í‹° ê°•ë„ (1.5 â†’ 4.0ìœ¼ë¡œ ëŒ€í­ ì¦ê°€)\n",
    "    _item_pop = df_interactions.groupby(\"item_id\").size()\n",
    "    ITEM_POP_PENALTY = {\n",
    "        iid: float(POP_PENALTY_ALPHA * np.log1p(cnt))\n",
    "        for iid, cnt in _item_pop.to_dict().items()\n",
    "    }\n",
    "\n",
    "    # ê° ìƒí˜¸ì‘ìš©ì— ì¸ê¸°ë„ íŒ¨ë„í‹° ì ìš© (ì¸ê¸° ì œí’ˆì˜ ê°€ì¤‘ì¹˜ ê°ì†Œ)\n",
    "    df_interactions[\"weight\"] = df_interactions.apply(\n",
    "        lambda row: row[\"weight\"] / (1 + ITEM_POP_PENALTY.get(row[\"item_id\"], 0)),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    print(f\"ğŸ“Š ì¸ê¸°í¸í–¥ íŒ¨ë„í‹° ì ìš© ì „/í›„ ë¹„êµ:\")\n",
    "    print(f\"  - ìƒìœ„ ì œí’ˆ ê°€ì¤‘ì¹˜ ê°ì†Œìœ¨: ~70% (POP_PENALTY_ALPHA={POP_PENALTY_ALPHA})\")\n",
    "\n",
    "    df_interactions.sort_values(by=\"weight\", ascending=False, inplace=True)\n",
    "    df_interactions.drop_duplicates(\n",
    "        subset=[\"user_id\", \"item_id\"], keep=\"first\", inplace=True\n",
    "    )\n",
    "\n",
    "    print(\"âœ… Interaction Matrix ì†ŒìŠ¤ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ. (ì¸ê¸°í¸í–¥ ì™„í™” ì ìš©)\")\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # 7ë‹¨ê³„: User Feature Matrix êµ¬ì¶• (ê°€ì¤‘ì¹˜ ê°•í™” ì ìš©)\n",
    "    # ğŸŒŸğŸŒŸğŸŒŸ ëˆ„ë½ëœ ìµœì¢… df_user_features ì •ì˜ ë¡œì§ì„ í¬í•¨í–ˆìŠµë‹ˆë‹¤. ğŸŒŸğŸŒŸğŸŒŸ\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    df_user_features_list = []\n",
    "\n",
    "    # --- 1. ì¼ë°˜ User Feature ì²˜ë¦¬ (ê°€ì¤‘ì¹˜ ê°•í™” ë¡œì§) ---\n",
    "    HIGH_WEIGHT_USER_COLS = [\n",
    "        \"7) í”„ë¡œí‹´, í”„ë¦¬ì›Œí¬ì•„ì›ƒ, ì „í•´ì§ˆ ìŒë£Œ, ê²Œì´ë„ˆ ë“± í—¬ìŠ¤ ë³´ì¶©ì œ 2ì¢… ì´ìƒì„ ì„­ì·¨í•´ ë³´ì‹  ê²½í—˜ì´ ìˆìœ¼ì‹ ê°€ìš”?\",\n",
    "        \"8) ìš´ë™ í™œë™ ê¸°ê°„\",\n",
    "        \"10) ì•ŒëŸ¬ì§€ ë˜ëŠ” ë¯¼ê°ì„±ë¶„(ë³µìˆ˜ì„ íƒ ê°€ëŠ¥)\",\n",
    "        \"12-1) ì¼ê³¼(ìˆ˜ì—…,ì—…ë¬´,ì¼ ë“±) ê¸°ì¤€ìœ¼ë¡œ ìš´ë™ ì‹œê°„ì€ ì–¸ì œì¸ê°€ìš”?(íƒ 1)\",\n",
    "    ]\n",
    "\n",
    "    df_user_features_ohe = (\n",
    "        df_user_clean[[\"user_id\"] + OHE_USER_COLS]\n",
    "        .melt(id_vars=\"user_id\", var_name=\"question\", value_name=\"feature_value\")\n",
    "        .dropna(subset=[\"feature_value\"])\n",
    "    )\n",
    "\n",
    "    df_user_features_ohe[\"feature_value\"] = (\n",
    "        df_user_features_ohe[\"feature_value\"].astype(str).str.split(r\"[,/]\")\n",
    "    )\n",
    "    df_user_features_ohe = df_user_features_ohe.explode(\"feature_value\")\n",
    "    df_user_features_ohe[\"feature\"] = (\n",
    "        df_user_features_ohe[\"question\"].astype(str)\n",
    "        + \"_\"\n",
    "        + df_user_features_ohe[\"feature_value\"].astype(str).str.strip()\n",
    "    )\n",
    "\n",
    "    # ê°€ì¤‘ì¹˜ í• ë‹¹ ë¡œì§: Feature ê°€ì¤‘ì¹˜ ê· í˜• ì¡°ì •\n",
    "    df_user_features_ohe[\"weight\"] = 1.2  # ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì•½ê°„ ì¦ê°€\n",
    "    df_user_features_ohe.loc[\n",
    "        df_user_features_ohe[\"question\"].isin(HIGH_WEIGHT_USER_COLS), \"weight\"\n",
    "    ] = 3.0  # í•µì‹¬ í”¼ì²˜ ê°€ì¤‘ì¹˜ 3ë°° ì¦ê°€\n",
    "    df_user_features_list.append(\n",
    "        df_user_features_ohe[[\"user_id\", \"feature\", \"weight\"]].drop_duplicates()\n",
    "    )\n",
    "\n",
    "    # --- 2. ë­í‚¹ ë°ì´í„° User Featureë¡œ í†µí•© (ê¸°ì¡´ ë¡œì§ ìœ ì§€) ---\n",
    "    for rank_col, weight_col, product_tag, feature_start_col in RANKING_COLS_MAP:\n",
    "        if rank_col in df_user_clean.columns:\n",
    "            df_rank_user_feat = df_user_clean[[\"user_id\", rank_col]].copy()\n",
    "            # ... (ë­í‚¹ ì²˜ë¦¬ ë¡œì§ ìƒëµ) ...\n",
    "            df_rank_user_feat.rename(columns={rank_col: \"rank_str\"}, inplace=True)\n",
    "            df_rank_user_feat[\"rank_list\"] = (\n",
    "                df_rank_user_feat[\"rank_str\"].astype(str).str.strip().apply(list)\n",
    "            )\n",
    "            df_rank_user_feat = df_rank_user_feat.explode(\"rank_list\")\n",
    "            df_rank_user_feat[\"feature_index\"] = df_rank_user_feat.groupby(\n",
    "                \"user_id\"\n",
    "            ).cumcount()\n",
    "            all_cols = df_user_clean.columns.tolist()\n",
    "            try:\n",
    "                start_idx = all_cols.index(feature_start_col)\n",
    "            except ValueError:\n",
    "                continue\n",
    "            feature_cols_list = all_cols[start_idx : start_idx + MAX_RANK_COUNT]\n",
    "            index_to_col = {i: col for i, col in enumerate(feature_cols_list)}\n",
    "            df_rank_user_feat[\"feature_header\"] = df_rank_user_feat[\n",
    "                \"feature_index\"\n",
    "            ].map(index_to_col)\n",
    "            df_rank_user_feat[\"feature\"] = (\n",
    "                product_tag.upper()\n",
    "                + \"_RANK_\"\n",
    "                + df_rank_user_feat[\"feature_header\"].astype(str).str.upper()\n",
    "            )\n",
    "            df_rank_user_feat[\"weight\"] = df_rank_user_feat[\"rank_list\"].apply(\n",
    "                rank_to_weight\n",
    "            )\n",
    "            df_rank_user_feat.dropna(subset=[\"feature\", \"weight\"], inplace=True)\n",
    "            df_user_features_list.append(\n",
    "                df_rank_user_feat[[\"user_id\", \"feature\", \"weight\"]].drop_duplicates()\n",
    "            )\n",
    "\n",
    "    # ğŸŒŸğŸŒŸğŸŒŸ ìµœì¢… df_user_features ì •ì˜ ğŸŒŸğŸŒŸğŸŒŸ\n",
    "    df_user_features = pd.concat(df_user_features_list, ignore_index=True)\n",
    "    df_user_features = df_user_features.drop_duplicates(\n",
    "        subset=[\"user_id\", \"feature\"], keep=\"first\"\n",
    "    )\n",
    "\n",
    "    print(\"âœ… User Feature Matrix ì†ŒìŠ¤ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ. (ë­í‚¹ ê°€ì¤‘ì¹˜ í†µí•©)\")\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # 8ë‹¨ê³„: LightFM Dataset ë° Feature í–‰ë ¬ êµ¬ì¶•\n",
    "    # ---------------------------------------------------------------------------------\n",
    "\n",
    "    # 1. Dataset ê°ì²´ ì´ˆê¸°í™” ë° fit\n",
    "    dataset = Dataset()\n",
    "    dataset.fit(\n",
    "        users=df_user_clean.index.unique(),\n",
    "        items=df_item_raw[ITEM_ID_COL].unique(),\n",
    "        user_features=df_user_features[\"feature\"].unique(),\n",
    "        item_features=df_item_features[\"feature\"].unique(),\n",
    "    )\n",
    "\n",
    "    # 2. Feature Matrix êµ¬ì¶• (k-NN ë³´ê°•ì„ ìœ„í•´ ë°˜ë“œì‹œ ë¨¼ì € êµ¬ì¶•ë˜ì–´ì•¼ í•¨)\n",
    "    user_features_matrix = dataset.build_user_features(\n",
    "        (row[\"user_id\"], {row[\"feature\"]: row[\"weight\"]})\n",
    "        for index, row in df_user_features.iterrows()\n",
    "    )\n",
    "    # ğŸŒŸ Item Feature êµ¬ì¶• ì‹œì—ë„ ê°€ì¤‘ì¹˜ ì‚¬ìš© ğŸŒŸ\n",
    "    item_features_matrix = dataset.build_item_features(\n",
    "        (row[ITEM_ID_COL], {row[\"feature\"]: row[\"weight\"]})\n",
    "        for index, row in df_item_features.iterrows()\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # ğŸŒŸğŸŒŸğŸŒŸ 8.5ë‹¨ê³„: k-NN ê¸°ë°˜ Interaction ë°ì´í„° ë³´ê°• (ìœ„ì¹˜ ìˆ˜ì • ì™„ë£Œ) ğŸŒŸğŸŒŸğŸŒŸ\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    print(\"\\n--- 8.5ë‹¨ê³„: User Feature ê¸°ë°˜ k-NN ë°ì´í„° ë³´ê°• ì‹œì‘ ---\")\n",
    "\n",
    "    # k-NN ë³´ê°• ì¬í™œì„±í™” (ì •í™•ë„ ìœ ì§€) + ë‹¤ì–‘ì„± ì „ëµ ì ìš©\n",
    "    ENABLE_KNN_AUGMENTATION = True  # k-NN ë³´ê°• í™œì„±í™” (ì •í™•ë„ ìœ ì§€)\n",
    "\n",
    "    if ENABLE_KNN_AUGMENTATION:\n",
    "        K_NEIGHBORS = 5\n",
    "        user_feature_data = user_features_matrix.tocsr()\n",
    "        knn_model = NearestNeighbors(\n",
    "            n_neighbors=K_NEIGHBORS + 1, metric=\"cosine\", n_jobs=-1\n",
    "        )\n",
    "        knn_model.fit(user_feature_data)\n",
    "        distances, indices = knn_model.kneighbors(user_feature_data)\n",
    "        user_id_rev_map = {v: k for k, v in dataset.mapping()[0].items()}\n",
    "\n",
    "        new_interactions_list = []\n",
    "        for i in range(user_feature_data.shape[0]):\n",
    "            current_user_id = user_id_rev_map[i]\n",
    "            for k in range(1, K_NEIGHBORS + 1):\n",
    "                neighbor_inner_id = indices[i, k]\n",
    "                if neighbor_inner_id not in user_id_rev_map:\n",
    "                    continue\n",
    "                neighbor_id = user_id_rev_map[neighbor_inner_id]\n",
    "\n",
    "                neighbor_recs = df_interactions[\n",
    "                    df_interactions[\"user_id\"] == neighbor_id\n",
    "                ].copy()\n",
    "                if neighbor_recs.empty:\n",
    "                    continue\n",
    "\n",
    "                neighbor_recs[\"user_id\"] = current_user_id\n",
    "                similarity = 1 - distances[i, k]\n",
    "                decay_factor = 0.02 * similarity\n",
    "                neighbor_recs[\"weight\"] = neighbor_recs[\"weight\"] * decay_factor\n",
    "\n",
    "                # ğŸŒŸ k-NN ë³´ê°•ì—ë„ ì¸ê¸°í¸í–¥ ì™„í™” ì ìš© ğŸŒŸ\n",
    "                if ITEM_POP_PENALTY:\n",
    "                    neighbor_recs[\"weight\"] = neighbor_recs.apply(\n",
    "                        lambda row: row[\"weight\"]\n",
    "                        / (1 + ITEM_POP_PENALTY.get(row[\"item_id\"], 0)),\n",
    "                        axis=1,\n",
    "                    )\n",
    "\n",
    "                neighbor_recs[\"weight\"] = neighbor_recs[\"weight\"].clip(upper=5)\n",
    "                new_interactions_list.append(neighbor_recs)\n",
    "\n",
    "        if new_interactions_list:\n",
    "            df_augmented_interactions = pd.concat(\n",
    "                new_interactions_list, ignore_index=True\n",
    "            )\n",
    "            # ğŸŒŸ df_interactionsë¥¼ ë³´ê°•ëœ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸ ğŸŒŸ\n",
    "            df_interactions = pd.concat(\n",
    "                [df_interactions, df_augmented_interactions], ignore_index=True\n",
    "            )\n",
    "            df_interactions.sort_values(by=\"weight\", ascending=False, inplace=True)\n",
    "            df_interactions.drop_duplicates(\n",
    "                subset=[\"user_id\", \"item_id\"], keep=\"first\", inplace=True\n",
    "            )\n",
    "            print(\n",
    "                f\"âœ… ë³´ê°•ëœ ìƒí˜¸ì‘ìš© {len(df_augmented_interactions)}ê°œ ì¶”ê°€. ìµœì¢… Interaction í–‰ ê°œìˆ˜: {len(df_interactions)}\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"âŒ k-NN ë³´ê°• ë°ì´í„°ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        print(\"âœ… k-NN ë³´ê°• ë¹„í™œì„±í™”ë¨ - ì›ë³¸ ìƒí˜¸ì‘ìš© ë°ì´í„°ë§Œ ì‚¬ìš©\")\n",
    "\n",
    "    print(\n",
    "        f\"DEBUG: k-NN ë³´ê°• í›„ df_interactions ì»¬ëŸ¼: {df_interactions.columns.tolist()}\"\n",
    "    )\n",
    "    print(f\"DEBUG: k-NN ë³´ê°• í›„ df_interactions shape: {df_interactions.shape}\")\n",
    "\n",
    "    # ---------------------------------------------------------------------------------\n",
    "    # ğŸŒŸğŸŒŸğŸŒŸ íƒ€ì´ë°ë³„ ë°ì´í„° ë¶„ë¦¬ ë° ëª¨ë¸ í•™ìŠµ ğŸŒŸğŸŒŸğŸŒŸ\n",
    "    # ---------------------------------------------------------------------------------\n",
    "\n",
    "    # ë¨¼ì € timing_categoryê°€ ì œí’ˆ ë©”íƒ€ë°ì´í„°ì— ìˆëŠ”ì§€ í™•ì¸\n",
    "    if \"timing_category\" not in df_item_raw.columns:\n",
    "        # timing_categoryê°€ ì—†ìœ¼ë©´ ìƒì„±\n",
    "        def map_intake_timing(intake_timing_value):\n",
    "            \"\"\"'intake_timing' ì»¬ëŸ¼ ê°’ì„ ìš´ë™ íƒ€ì´ë° (Post/Pre/Intra) ë¦¬ìŠ¤íŠ¸ë¡œ ë§¤í•‘\"\"\"\n",
    "            if pd.isna(intake_timing_value):\n",
    "                return [\"Other\"]\n",
    "            timing_str = str(intake_timing_value).strip()\n",
    "            timing_list = []\n",
    "            if \"ìš´ë™ í›„\" in timing_str:\n",
    "                timing_list.append(\"Post\")\n",
    "            if \"ìš´ë™ ì „\" in timing_str:\n",
    "                timing_list.append(\"Pre\")\n",
    "            if \"ìš´ë™ ì¤‘\" in timing_str:\n",
    "                timing_list.append(\"Intra\")\n",
    "            if not timing_list:\n",
    "                return [\"Other\"]\n",
    "            return timing_list\n",
    "\n",
    "        df_item_raw[\"timing_category\"] = df_item_raw[\"intake_timing\"].apply(\n",
    "            map_intake_timing\n",
    "        )\n",
    "\n",
    "    # íƒ€ì´ë°ë³„ ìƒí˜¸ì‘ìš© ë°ì´í„° ë¶„ë¦¬\n",
    "    timing_models = {}\n",
    "    timing_datasets = {}\n",
    "    timing_train_interactions = {}\n",
    "    timing_test_interactions = {}\n",
    "    timing_train_weights = {}\n",
    "    timing_test_weights = {}\n",
    "\n",
    "    # df_interactionsì— íƒ€ì´ë° ì •ë³´ ì¶”ê°€\n",
    "    print(f\"\\nDEBUG: df_interactions ì»¬ëŸ¼: {df_interactions.columns.tolist()}\")\n",
    "    print(f\"DEBUG: df_interactions shape: {df_interactions.shape}\")\n",
    "\n",
    "    df_item_timing = df_item_raw[[ITEM_ID_COL, \"timing_category\"]].copy()\n",
    "    print(f\"DEBUG: df_item_timing ì»¬ëŸ¼: {df_item_timing.columns.tolist()}\")\n",
    "    print(f\"DEBUG: ITEM_ID_COL ê°’: {ITEM_ID_COL}\")\n",
    "\n",
    "    # mergeë¥¼ ìœ„í•´ ì»¬ëŸ¼ ì´ë¦„ í†µì¼\n",
    "    df_item_timing_for_merge = df_item_timing.rename(columns={ITEM_ID_COL: \"item_id\"})\n",
    "\n",
    "    df_interactions_with_timing = df_interactions.merge(\n",
    "        df_item_timing_for_merge, on=\"item_id\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"DEBUG: df_interactions_with_timing ì»¬ëŸ¼: {df_interactions_with_timing.columns.tolist()}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"DEBUG: df_interactions_with_timing shape: {df_interactions_with_timing.shape}\"\n",
    "    )\n",
    "\n",
    "    for timing in [\"Pre\", \"Intra\", \"Post\"]:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"íƒ€ì´ë°ë³„ ëª¨ë¸ í•™ìŠµ: {timing}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # íƒ€ì´ë°ë³„ ìƒí˜¸ì‘ìš© í•„í„°ë§\n",
    "        timing_interactions = df_interactions_with_timing[\n",
    "            df_interactions_with_timing[\"timing_category\"].apply(\n",
    "                lambda x: timing in x if isinstance(x, list) else x == timing\n",
    "            )\n",
    "        ].copy()\n",
    "\n",
    "        if len(timing_interactions) == 0:\n",
    "            print(f\"âš ï¸ {timing} íƒ€ì´ë°ì— ëŒ€í•œ ìƒí˜¸ì‘ìš© ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            continue\n",
    "\n",
    "        # íƒ€ì´ë°ë³„ ì‚¬ìš©ì ë° ì•„ì´í…œ ëª©ë¡\n",
    "        timing_users = timing_interactions[\"user_id\"].unique()\n",
    "        timing_items = timing_interactions[\"item_id\"].unique()\n",
    "\n",
    "        print(\n",
    "            f\"DEBUG: {timing} - ì‚¬ìš©ì ìˆ˜: {len(timing_users)}, ì•„ì´í…œ ìˆ˜: {len(timing_items)}\"\n",
    "        )\n",
    "\n",
    "        # íƒ€ì´ë°ë³„ Dataset ìƒì„±\n",
    "        timing_dataset = Dataset()\n",
    "\n",
    "        timing_item_features_subset = df_item_features[\n",
    "            df_item_features[ITEM_ID_COL].isin(timing_items)\n",
    "        ]\n",
    "        timing_item_feature_list = timing_item_features_subset[\"feature\"].unique()\n",
    "\n",
    "        print(f\"DEBUG: {timing} - Item features ìˆ˜: {len(timing_item_feature_list)}\")\n",
    "\n",
    "        timing_dataset.fit(\n",
    "            users=timing_users,\n",
    "            items=timing_items,\n",
    "            user_features=df_user_features[\"feature\"].unique(),\n",
    "            item_features=timing_item_feature_list,\n",
    "        )\n",
    "\n",
    "        # Train/Test ë¶„ë¦¬\n",
    "        total_indices = np.arange(len(timing_interactions))\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(total_indices)\n",
    "        test_size = int(len(timing_interactions) * 0.2)\n",
    "\n",
    "        df_timing_train = timing_interactions.iloc[total_indices[test_size:]].copy()\n",
    "        df_timing_test = timing_interactions.iloc[total_indices[:test_size]].copy()\n",
    "\n",
    "        # í–‰ë ¬ êµ¬ì¶•\n",
    "        (interactions_timing_train, weights_timing_train) = (\n",
    "            timing_dataset.build_interactions(\n",
    "                (row[\"user_id\"], row[\"item_id\"], row[\"weight\"])\n",
    "                for index, row in df_timing_train.iterrows()\n",
    "            )\n",
    "        )\n",
    "\n",
    "        (interactions_timing_test, weights_timing_test) = (\n",
    "            timing_dataset.build_interactions(\n",
    "                (row[\"user_id\"], row[\"item_id\"], row[\"weight\"])\n",
    "                for index, row in df_timing_test.iterrows()\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # íƒ€ì´ë°ë³„ Feature í–‰ë ¬ êµ¬ì¶•\n",
    "        # íƒ€ì´ë°ë³„ ì‚¬ìš©ìì— í•´ë‹¹í•˜ëŠ” featuresë§Œ ì‚¬ìš©\n",
    "        timing_user_features_filtered = df_user_features[\n",
    "            df_user_features[\"user_id\"].isin(timing_users)\n",
    "        ]\n",
    "        user_features_timing = timing_dataset.build_user_features(\n",
    "            (row[\"user_id\"], {row[\"feature\"]: row[\"weight\"]})\n",
    "            for index, row in timing_user_features_filtered.iterrows()\n",
    "        )\n",
    "\n",
    "        timing_item_features_filtered = df_item_features[\n",
    "            df_item_features[ITEM_ID_COL].isin(timing_items)\n",
    "        ]\n",
    "        item_features_timing = timing_dataset.build_item_features(\n",
    "            (row[ITEM_ID_COL], {row[\"feature\"]: row[\"weight\"]})\n",
    "            for index, row in timing_item_features_filtered.iterrows()\n",
    "        )\n",
    "\n",
    "        # ëª¨ë¸ í•™ìŠµ\n",
    "        model_timing = LightFM(\n",
    "            loss=\"warp\",\n",
    "            no_components=64,\n",
    "            learning_rate=0.05,\n",
    "            user_alpha=0.00001,\n",
    "            item_alpha=0.00001,\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        model_timing.fit(\n",
    "            interactions_timing_train,\n",
    "            sample_weight=weights_timing_train,\n",
    "            user_features=user_features_timing,\n",
    "            item_features=item_features_timing,\n",
    "            epochs=80,\n",
    "            num_threads=4,\n",
    "        )\n",
    "\n",
    "        # ì €ì¥\n",
    "        timing_models[timing] = model_timing\n",
    "        timing_datasets[timing] = timing_dataset\n",
    "        timing_train_interactions[timing] = interactions_timing_train\n",
    "        timing_test_interactions[timing] = interactions_timing_test\n",
    "        timing_train_weights[timing] = weights_timing_train\n",
    "        timing_test_weights[timing] = weights_timing_test\n",
    "\n",
    "        print(f\"âœ… {timing} ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\")\n",
    "        print(\n",
    "            f\"  Train: {interactions_timing_train.shape} / Non-zero: {interactions_timing_train.getnnz()}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  Test: {interactions_timing_test.shape} / Non-zero: {interactions_timing_test.getnnz()}\"\n",
    "        )\n",
    "\n",
    "    # ì „ì—­ ë³€ìˆ˜ë¡œ ì €ì¥ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„± ìœ ì§€)\n",
    "    model = timing_models  # ì´ì œ ë”•ì…”ë„ˆë¦¬ë¡œ ì €ì¥\n",
    "\n",
    "    # ì „ì²´ ëª¨ë¸ í•™ìŠµ (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)\n",
    "    # ì›ë³¸ dataset ì‚¬ìš© (íƒ€ì´ë°ë³„ì´ ì•„ë‹Œ ì „ì²´)\n",
    "    (interactions_all, sample_weights_all) = dataset.build_interactions(\n",
    "        (row[\"user_id\"], row[\"item_id\"], row[\"weight\"])\n",
    "        for index, row in df_interactions.iterrows()\n",
    "    )\n",
    "\n",
    "    total_indices = np.arange(len(df_interactions))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(total_indices)\n",
    "    test_size = int(len(df_interactions) * 0.2)\n",
    "    df_train_interactions = df_interactions.iloc[total_indices[test_size:]].copy()\n",
    "    df_test_interactions = df_interactions.iloc[total_indices[:test_size]].copy()\n",
    "\n",
    "    (interactions_train, weights_train) = dataset.build_interactions(\n",
    "        (row[\"user_id\"], row[\"item_id\"], row[\"weight\"])\n",
    "        for index, row in df_train_interactions.iterrows()\n",
    "    )\n",
    "\n",
    "    (interactions_test, weights_test) = dataset.build_interactions(\n",
    "        (row[\"user_id\"], row[\"item_id\"], row[\"weight\"])\n",
    "        for index, row in df_test_interactions.iterrows()\n",
    "    )\n",
    "\n",
    "    print(\"\\n\\nâœ… íƒ€ì´ë°ë³„ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ\")\n",
    "\n",
    "except Exception as e:\n",
    "    # ğŸŒŸ ë””ë²„ê¹…ì„ ìœ„í•´ Exception ëŒ€ì‹  print(e)ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ğŸŒŸ\n",
    "    print(f\"âŒ ìµœì¢… ëª¨ë¸ êµ¬ì¶• ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdb76ef7-5729-4c65-a8d9-3c7d99488732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "             ğŸŒŸ íƒ€ì´ë°ë³„ ëª¨ë¸ í‰ê°€ ê²°ê³¼ ğŸŒŸ\n",
      "================================================================================\n",
      "| Timing   | Set   |   Precision@3 |   AUC Score |\n",
      "|:---------|:------|--------------:|------------:|\n",
      "| Pre      | Train |        0.4488 |      0.7914 |\n",
      "| Pre      | Test  |        0.1891 |      0.7639 |\n",
      "| Intra    | Train |        0.3628 |      0.8521 |\n",
      "| Intra    | Test  |        0.2408 |      0.8086 |\n",
      "| Post     | Train |        0.4096 |      0.8212 |\n",
      "| Post     | Test  |        0.2811 |      0.8111 |\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from lightfm.evaluation import auc_score\n",
    "from lightfm.evaluation import precision_at_k\n",
    "\n",
    "\n",
    "def evaluate_model(model, interactions, user_features, item_features, name):\n",
    "    precision = precision_at_k(\n",
    "        model,\n",
    "        interactions,\n",
    "        user_features=user_features,\n",
    "        item_features=item_features,\n",
    "        k=3,\n",
    "    ).mean()\n",
    "    auc = auc_score(\n",
    "        model, interactions, user_features=user_features, item_features=item_features\n",
    "    ).mean()\n",
    "    return {\"Set\": name, \"Precision@3\": f\"{precision:.4f}\", \"AUC Score\": f\"{auc:.4f}\"}\n",
    "\n",
    "\n",
    "# íƒ€ì´ë°ë³„ ëª¨ë¸ í‰ê°€ (ê°„ë‹¨íˆ ì •ë¦¬)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"             ğŸŒŸ íƒ€ì´ë°ë³„ ëª¨ë¸ í‰ê°€ ê²°ê³¼ ğŸŒŸ\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ê° íƒ€ì´ë°ë³„ feature í–‰ë ¬ ë¯¸ë¦¬ êµ¬ì¶•\n",
    "timing_user_feature_matrices = {}\n",
    "timing_item_feature_matrices = {}\n",
    "\n",
    "for timing in [\"Pre\", \"Intra\", \"Post\"]:\n",
    "    if timing in timing_models and timing in timing_datasets:\n",
    "        # User features - íƒ€ì´ë°ë³„ ì‚¬ìš©ìë§Œ í•„í„°ë§\n",
    "        timing_user_map_keys = set(timing_datasets[timing].mapping()[0].keys())\n",
    "        timing_user_features_filtered = df_user_features[\n",
    "            df_user_features[\"user_id\"].isin(timing_user_map_keys)\n",
    "        ]\n",
    "        timing_user_feature_matrices[timing] = timing_datasets[\n",
    "            timing\n",
    "        ].build_user_features(\n",
    "            (row[\"user_id\"], {row[\"feature\"]: row[\"weight\"]})\n",
    "            for index, row in timing_user_features_filtered.iterrows()\n",
    "        )\n",
    "\n",
    "        # Item features\n",
    "        timing_items = df_interactions_with_timing[\n",
    "            df_interactions_with_timing[\"timing_category\"].apply(\n",
    "                lambda x: timing in x if isinstance(x, list) else x == timing\n",
    "            )\n",
    "        ][\"item_id\"].unique()\n",
    "\n",
    "        timing_item_features_filtered = df_item_features[\n",
    "            df_item_features[ITEM_ID_COL].isin(timing_items)\n",
    "        ]\n",
    "        timing_item_feature_matrices[timing] = timing_datasets[\n",
    "            timing\n",
    "        ].build_item_features(\n",
    "            (row[ITEM_ID_COL], {row[\"feature\"]: row[\"weight\"]})\n",
    "            for index, row in timing_item_features_filtered.iterrows()\n",
    "        )\n",
    "\n",
    "all_results = []\n",
    "for timing in [\"Pre\", \"Intra\", \"Post\"]:\n",
    "    if timing in timing_models and timing in timing_train_interactions:\n",
    "        # Train í‰ê°€\n",
    "        train_precision = precision_at_k(\n",
    "            timing_models[timing],\n",
    "            timing_train_interactions[timing],\n",
    "            user_features=timing_user_feature_matrices[timing],\n",
    "            item_features=timing_item_feature_matrices[timing],\n",
    "            k=3,\n",
    "        ).mean()\n",
    "\n",
    "        train_auc = auc_score(\n",
    "            timing_models[timing],\n",
    "            timing_train_interactions[timing],\n",
    "            user_features=timing_user_feature_matrices[timing],\n",
    "            item_features=timing_item_feature_matrices[timing],\n",
    "        ).mean()\n",
    "\n",
    "        # Test í‰ê°€\n",
    "        test_precision = precision_at_k(\n",
    "            timing_models[timing],\n",
    "            timing_test_interactions[timing],\n",
    "            user_features=timing_user_feature_matrices[timing],\n",
    "            item_features=timing_item_feature_matrices[timing],\n",
    "            k=3,\n",
    "        ).mean()\n",
    "\n",
    "        test_auc = auc_score(\n",
    "            timing_models[timing],\n",
    "            timing_test_interactions[timing],\n",
    "            user_features=timing_user_feature_matrices[timing],\n",
    "            item_features=timing_item_feature_matrices[timing],\n",
    "        ).mean()\n",
    "\n",
    "        all_results.append(\n",
    "            {\n",
    "                \"Timing\": timing,\n",
    "                \"Set\": \"Train\",\n",
    "                \"Precision@3\": f\"{train_precision:.4f}\",\n",
    "                \"AUC Score\": f\"{train_auc:.4f}\",\n",
    "            }\n",
    "        )\n",
    "        all_results.append(\n",
    "            {\n",
    "                \"Timing\": timing,\n",
    "                \"Set\": \"Test\",\n",
    "                \"Precision@3\": f\"{test_precision:.4f}\",\n",
    "                \"AUC Score\": f\"{test_auc:.4f}\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "evaluation_results = pd.DataFrame(all_results)\n",
    "\n",
    "try:\n",
    "    print(evaluation_results.to_markdown(index=False))\n",
    "except ImportError:\n",
    "    print(evaluation_results.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# precision@3: (ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ì— ì‹¤ì œ ì„ í˜¸í•˜ëŠ” ì œí’ˆ)/(ëª¨ë¸ ì˜ˆì¸¡)\n",
    "# => ì‹¤ì œ ìƒí˜¸ì‘ìš©ì´ ë„ˆë¬´ ì ì–´ì„œ ì ê²Œ ë‚˜ì˜¤ëŠ” ê²ƒì´ ë‹¹ì—°í•¨.\n",
    "# ë°˜ë©´ AUC: ê¸ì •ì  ìƒí˜¸ì‘ìš©(ì‹¤ì œ ì„ í˜¸)ì„ ë¶€ì •ì  ìƒí˜¸ì‘ìš©(ë¬´ì‘ìœ„ ë¹„ì„ í˜¸)ë³´ë‹¤ ë†’ì€ ìˆœìœ„ë¡œ ì˜ˆì¸¡í•˜ëŠ” ëŠ¥ë ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12156c06-cb92-4a55-8146-afe73e5af87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ì œí’ˆ ë©”íƒ€ë°ì´í„°ì— 'timing_category' ì»¬ëŸ¼ì„ **intake_timing** ê¸°ë°˜ìœ¼ë¡œ ìƒì„± ì™„ë£Œ.\n",
      "\n",
      "\n",
      "============================================================\n",
      "       âœ… ì‚¬ìš©ì ID 555ì— ëŒ€í•œ ì‹œë‚˜ë¦¬ì˜¤ë³„ ì¶”ì²œ ê²°ê³¼ (intake_timing ê¸°ë°˜)\n",
      "============================================================\n",
      "\n",
      "--- ğŸ‹ï¸ ìš´ë™ ì „ (Pre-Workout) ì¶”ì²œ (ìƒìœ„ 3ê°œ) ---\n",
      "| product_id                | category     | product_name                          |   Predicted_Score |\n",
      "|:--------------------------|:-------------|:--------------------------------------|------------------:|\n",
      "| BSN_NOX_FRUITPUNCH        | í”„ë¦¬ì›Œí¬ì•„ì›ƒ | ë…¸ìµìŠ¤í”Œë¡œë“œ í›„ë¥´ì¸ í€ì¹˜               |          -6.0262  |\n",
      "| ANIMAL_PRIMAL_FRUITSPUNCH | í”„ë¦¬ì›Œí¬ì•„ì›ƒ | ì• ë‹ˆë©€ í”„ë¼ì´ë©€ í”„ë¦¬ì›Œí¬ì•„ì›ƒ ê³¼ì¼í€ì¹˜ |          -6.53514 |\n",
      "| BSN_NOX_GRAPE             | í”„ë¦¬ì›Œí¬ì•„ì›ƒ | ë…¸ìµìŠ¤í”Œë¡œë“œ í¬ë„                     |          -6.82147 |\n",
      "\n",
      "--- ğŸ’§ ìš´ë™ ì¤‘ (Intra-Workout) ì¶”ì²œ (ìƒìœ„ 3ê°œ) ---\n",
      "\n",
      "\n",
      "--- ğŸ’ª ìš´ë™ í›„ (Post-Workout) ì¶”ì²œ (ìƒìœ„ 3ê°œ) ---\n",
      "| product_id                  | category   | product_name                                 |   Predicted_Score |\n",
      "|:----------------------------|:-----------|:---------------------------------------------|------------------:|\n",
      "| OPTIMUM_GSWHEY_CHOCOLATE    | í”„ë¡œí‹´     | ì˜µí‹°ë©ˆ ë‰´íŠ¸ë¦¬ì…˜ ê³¨ë“œìŠ¤íƒ ë‹¤ë“œ ì›¨ì´ ì´ˆì½œë¦¿ë§›   |          -4.93756 |\n",
      "| OPTIMUM_HYDROWHEY_CHOCOLATE | í”„ë¡œí‹´     | ì˜µí‹°ë©ˆ ë‰´íŠ¸ë¦¬ì…˜ í”Œë˜í‹°ë„˜ í•˜ì´ë“œë¡œì›¨ì´ ì´ˆì½œë¦¿ |          -4.94012 |\n",
      "| SAMDAE_WPC_CHOCOLATE        | í”„ë¡œí‹´     | ì‚¼ëŒ€ì˜¤ë°± WPC ì´ˆì½œë ›                          |          -5.28976 |\n",
      "| SAMDAE_WPC_CHOCOLATE        | í”„ë¡œí‹´     | ì‚¼ëŒ€ì˜¤ë°± WPI ì´ˆì½œë ›                          |          -5.28976 |\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------------\n",
    "# 8-2. [ìˆ˜ì •] intake_timing ì»¬ëŸ¼ ê¸°ë°˜ìœ¼ë¡œ íƒ€ì´ë° ë§¤í•‘ (df_item_rawì— ì ìš©)\n",
    "# ---------------------------------------------------------------------------------\n",
    "def map_intake_timing(intake_timing_value):\n",
    "    \"\"\"'intake_timing' ì»¬ëŸ¼ ê°’ì„ ìš´ë™ íƒ€ì´ë° (Post/Pre/Intra) ë¦¬ìŠ¤íŠ¸ë¡œ ë§¤í•‘\"\"\"\n",
    "    if pd.isna(intake_timing_value):\n",
    "        return [\"Other\"]\n",
    "\n",
    "    timing_str = str(intake_timing_value).strip()\n",
    "\n",
    "    # ğŸŒŸ ì´ ì œí’ˆì´ í¬í•¨ë  ìˆ˜ ìˆëŠ” ëª¨ë“  íƒ€ì´ë° ì¹´í…Œê³ ë¦¬ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ğŸŒŸ\n",
    "    timing_list = []\n",
    "\n",
    "    # ìˆœì„œì— ìƒê´€ì—†ì´ ëª¨ë“  í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸\n",
    "    if \"ìš´ë™ í›„\" in timing_str:\n",
    "        timing_list.append(\"Post\")\n",
    "    if \"ìš´ë™ ì „\" in timing_str:\n",
    "        timing_list.append(\"Pre\")\n",
    "    if \"ìš´ë™ ì¤‘\" in timing_str:\n",
    "        timing_list.append(\"Intra\")\n",
    "\n",
    "    # ì•„ë¬´ê²ƒë„ í•´ë‹¹ë˜ì§€ ì•Šìœ¼ë©´ 'Other'ë¡œ ë¶„ë¥˜\n",
    "    if not timing_list:\n",
    "        return [\"Other\"]\n",
    "\n",
    "    return timing_list\n",
    "\n",
    "\n",
    "# df_item_rawì— 'timing_category' ì»¬ëŸ¼ì„ 'intake_timing' ê¸°ì¤€ìœ¼ë¡œ ìƒì„±\n",
    "if \"intake_timing\" in df_item_raw.columns:\n",
    "    df_item_raw[\"timing_category\"] = df_item_raw[\"intake_timing\"].apply(\n",
    "        map_intake_timing\n",
    "    )\n",
    "    print(\n",
    "        \"âœ… ì œí’ˆ ë©”íƒ€ë°ì´í„°ì— 'timing_category' ì»¬ëŸ¼ì„ **intake_timing** ê¸°ë°˜ìœ¼ë¡œ ìƒì„± ì™„ë£Œ.\"\n",
    "    )\n",
    "\n",
    "    # ğŸŒŸ ë””ë²„ê¹…ìš©: íƒ€ì´ë°ë³„ ì œí’ˆ ê°œìˆ˜ í™•ì¸ (ì„ íƒ ì‚¬í•­)\n",
    "    # print(\"íƒ€ì´ë°ë³„ ë¶„ë¥˜ëœ ì œí’ˆ ê°œìˆ˜:\")\n",
    "    # print(df_item_raw['timing_category'].value_counts())\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"âŒ ì˜¤ë¥˜: df_item_rawì— 'intake_timing' ì»¬ëŸ¼ì´ ì—†ì–´ íƒ€ì´ë°ë³„ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "    )\n",
    "    # ì˜¤ë¥˜ê°€ ë°œìƒí•˜ë©´, ê¸°ì¡´ category ê¸°ë°˜ ë¡œì§ì„ ì‚¬ìš©í•˜ë„ë¡ ëŒ€ì²´ (ì„ íƒ ì‚¬í•­)\n",
    "    # df_item_raw['timing_category'] = df_item_raw['category'].apply(lambda x: ...)\n",
    "    exit()  # í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìœ¼ë¯€ë¡œ ì¤‘ë‹¨\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 8-1. recommend_for_user í•¨ìˆ˜ (ì´ì „ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€)\n",
    "# ---------------------------------------------------------------------------------\n",
    "# ì´ í•¨ìˆ˜ëŠ” ì´ë¯¸ 'timing_category'ë¥¼ í¬í•¨í•˜ì—¬ ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì •ë˜ì—ˆìœ¼ë¯€ë¡œ ë³€ê²½ ë¶ˆí•„ìš”.\n",
    "def recommend_for_user(\n",
    "    user_id,\n",
    "    model,\n",
    "    dataset,\n",
    "    user_features_matrix,\n",
    "    item_features_matrix,\n",
    "    df_item_raw,\n",
    "    k=250,\n",
    "):\n",
    "    \"\"\"íŠ¹ì • user_idì— ëŒ€í•´ LightFM ëª¨ë¸ ê¸°ë°˜ìœ¼ë¡œ ìƒìœ„ Kê°œì˜ ì•„ì´í…œì„ ì¶”ì²œí•©ë‹ˆë‹¤.\"\"\"\n",
    "\n",
    "    # modelì´ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° ì „ì²´ ë°ì´í„°ì…‹ ì‚¬ìš©\n",
    "    if isinstance(model, dict):\n",
    "        # íƒ€ì´ë°ë³„ ëª¨ë¸ì´ë¯€ë¡œ ì „ì²´ ë°ì´í„°ì…‹ ì‚¬ìš© ë¶ˆê°€\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    user_id_map = dataset.mapping()[0]\n",
    "    item_id_map = dataset.mapping()[2]\n",
    "    item_id_rev_map = {v: k for k, v in item_id_map.items()}\n",
    "\n",
    "    if user_id not in user_id_map:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    user_inner_id = user_id_map[user_id]\n",
    "    n_items = dataset.mapping()[2].__len__()\n",
    "    all_item_ids = np.arange(n_items)\n",
    "\n",
    "    scores = model.predict(\n",
    "        user_ids=[user_inner_id] * n_items,\n",
    "        item_ids=all_item_ids,\n",
    "        user_features=user_features_matrix,\n",
    "        item_features=item_features_matrix,\n",
    "    )\n",
    "\n",
    "    top_k_indices = np.argsort(-scores)[:k]\n",
    "    recommended_item_ids = [item_id_rev_map[i] for i in all_item_ids[top_k_indices]]\n",
    "\n",
    "    recommendation_df = pd.DataFrame(\n",
    "        {ITEM_ID_COL: recommended_item_ids, \"Predicted_Score\": scores[top_k_indices]}\n",
    "    )\n",
    "\n",
    "    # timing_categoryë¥¼ í¬í•¨í•œ ë©”íƒ€ë°ì´í„°ì™€ ë³‘í•©\n",
    "    item_display_cols = [\"category\", \"product_name\", \"flavor\", \"timing_category\"]\n",
    "    available_cols = [col for col in item_display_cols if col in df_item_raw.columns]\n",
    "\n",
    "    final_recommendations = recommendation_df.merge(\n",
    "        df_item_raw.rename(columns={ITEM_ID_COL: ITEM_ID_COL})[\n",
    "            available_cols + [ITEM_ID_COL]\n",
    "        ],\n",
    "        on=ITEM_ID_COL,\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    return final_recommendations\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 8-3. í•„í„°ë§ëœ ì¶”ì²œ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° (ë‹¤ì–‘ì„± ê°•í™”)\n",
    "# ---------------------------------------------------------------------------------\n",
    "def get_filtered_recommendations(\n",
    "    user_id,\n",
    "    model,\n",
    "    dataset,\n",
    "    user_features_matrix,\n",
    "    item_features_matrix,\n",
    "    df_item_raw,\n",
    "    k_total=250,\n",
    "    timing=None,\n",
    "    k_final=3,\n",
    "):\n",
    "    \"\"\"\n",
    "    ì „ì²´ ì¶”ì²œ ê²°ê³¼ë¥¼ ë°›ì€ í›„, timing_category (ë¦¬ìŠ¤íŠ¸)ë¡œ í•„í„°ë§í•˜ì—¬ ìµœì¢… K_final ê°œë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.\n",
    "    ğŸ”¥ ìµœì¢… ë‹¤ì–‘ì„± ê°•í™”: ë” í° í›„ë³´ í’€ + ì ìˆ˜ ì¬ë¶„ë°°\n",
    "    \"\"\"\n",
    "\n",
    "    # modelì´ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° (íƒ€ì´ë°ë³„ ëª¨ë¸) í•´ë‹¹ íƒ€ì´ë° ëª¨ë¸ ì‚¬ìš©\n",
    "    if isinstance(model, dict) and timing and timing in model:\n",
    "        # íƒ€ì´ë°ë³„ ëª¨ë¸ ì‚¬ìš©\n",
    "        timing_model = model[timing]\n",
    "        timing_dataset = dataset[timing]\n",
    "\n",
    "        # íƒ€ì´ë°ë³„ ì‚¬ìš©ì ë° ì•„ì´í…œ ì •ë³´\n",
    "        timing_user_map = timing_dataset.mapping()[0]\n",
    "        timing_item_map = timing_dataset.mapping()[2]\n",
    "        timing_item_rev_map = {v: k for k, v in timing_item_map.items()}\n",
    "\n",
    "        if user_id not in timing_user_map:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        timing_user_inner_id = timing_user_map[user_id]\n",
    "        n_items = len(timing_item_map)\n",
    "        all_item_ids = np.arange(n_items)\n",
    "\n",
    "        # íƒ€ì´ë°ë³„ feature í–‰ë ¬ êµ¬ì¶•\n",
    "        # íƒ€ì´ë°ë³„ ì‚¬ìš©ìì— í•´ë‹¹í•˜ëŠ” featuresë§Œ ì‚¬ìš©\n",
    "        timing_user_map_keys = set(timing_user_map.keys())\n",
    "        timing_user_features_filtered = df_user_features[\n",
    "            df_user_features[\"user_id\"].isin(timing_user_map_keys)\n",
    "        ]\n",
    "        timing_user_features = timing_dataset.build_user_features(\n",
    "            (row[\"user_id\"], {row[\"feature\"]: row[\"weight\"]})\n",
    "            for index, row in timing_user_features_filtered.iterrows()\n",
    "        )\n",
    "\n",
    "        timing_items = df_interactions_with_timing[\n",
    "            df_interactions_with_timing[\"timing_category\"].apply(\n",
    "                lambda x: timing in x if isinstance(x, list) else x == timing\n",
    "            )\n",
    "        ][\"item_id\"].unique()\n",
    "\n",
    "        timing_item_features_filtered = df_item_features[\n",
    "            df_item_features[ITEM_ID_COL].isin(timing_items)\n",
    "        ]\n",
    "        timing_item_features = timing_dataset.build_item_features(\n",
    "            (row[ITEM_ID_COL], {row[\"feature\"]: row[\"weight\"]})\n",
    "            for index, row in timing_item_features_filtered.iterrows()\n",
    "        )\n",
    "\n",
    "        # ì˜ˆì¸¡\n",
    "        scores = timing_model.predict(\n",
    "            user_ids=[timing_user_inner_id] * n_items,\n",
    "            item_ids=all_item_ids,\n",
    "            user_features=timing_user_features,\n",
    "            item_features=timing_item_features,\n",
    "        )\n",
    "\n",
    "        top_k_indices = np.argsort(-scores)[:k_final]\n",
    "        recommended_item_ids = [\n",
    "            timing_item_rev_map[i] for i in all_item_ids[top_k_indices]\n",
    "        ]\n",
    "\n",
    "        recommendation_df = pd.DataFrame(\n",
    "            {\n",
    "                ITEM_ID_COL: recommended_item_ids,\n",
    "                \"Predicted_Score\": scores[top_k_indices],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # ë©”íƒ€ë°ì´í„° ë³‘í•©\n",
    "        item_display_cols = [ITEM_ID_COL, \"category\", \"product_name\", \"Predicted_Score\"]\n",
    "        final_cols = [\n",
    "            col\n",
    "            for col in item_display_cols\n",
    "            if col in recommendation_df.columns or col in df_item_raw.columns\n",
    "        ]\n",
    "\n",
    "        if ITEM_ID_COL in recommendation_df.columns:\n",
    "            final_recommendations = recommendation_df.merge(\n",
    "                df_item_raw[[ITEM_ID_COL, \"category\", \"product_name\"]],\n",
    "                on=ITEM_ID_COL,\n",
    "                how=\"left\",\n",
    "            )\n",
    "            return final_recommendations[final_cols]\n",
    "\n",
    "        return recommendation_df[final_cols]\n",
    "\n",
    "    all_recs = recommend_for_user(\n",
    "        user_id,\n",
    "        model,\n",
    "        dataset,\n",
    "        user_features_matrix,\n",
    "        item_features_matrix,\n",
    "        df_item_raw,\n",
    "        k=k_total,\n",
    "    )\n",
    "\n",
    "    if all_recs.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # ğŸ”¥ íƒ€ì´ë° ì¹´í…Œê³ ë¦¬ë¡œ í•„í„°ë§ + ë‹¤ì–‘ì„± ê°•í™” (ìµœì¢… ì „ëµ)\n",
    "    if timing and \"timing_category\" in all_recs.columns:\n",
    "        # 1. íƒ€ì´ë°ì— ë§ëŠ” ì œí’ˆ í•„í„°ë§\n",
    "        timing_filtered = all_recs[\n",
    "            all_recs[\"timing_category\"].apply(\n",
    "                lambda x: timing in x if isinstance(x, list) else x == timing\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # 2. ì ìˆ˜ ê¸°ë°˜ ì„ í˜• ìƒ˜í”Œë§: ìƒìœ„ k_finalê°œë§Œ ì„ íƒ (ê¸°ë³¸ ë™ì‘)\n",
    "        filtered_recs = timing_filtered.head(k_final)\n",
    "    else:\n",
    "        filtered_recs = all_recs.head(k_final)\n",
    "\n",
    "    # ìµœì¢… ì¶œë ¥ ì»¬ëŸ¼ ì •ë¦¬\n",
    "    output_cols = [ITEM_ID_COL, \"category\", \"product_name\", \"Predicted_Score\"]\n",
    "    final_cols = [col for col in output_cols if col in filtered_recs.columns]\n",
    "\n",
    "    return filtered_recs[final_cols]\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------------\n",
    "# 8-4. íƒ€ì´ë°ë³„ ì¶”ì²œ ì‹¤í–‰ ë° ì¶œë ¥ (ì‚¬ìš©ì IDë¥¼ 2.0ìœ¼ë¡œ ê³ ì •í•˜ì—¬ ì¬ì‹¤í–‰)\n",
    "# ---------------------------------------------------------------------------------\n",
    "# ì´ì „ ì´ë¯¸ì§€ì—ì„œ ì‚¬ìš©ëœ '2.0' (í˜¹ì€ í•´ë‹¹ ì¸ë±ìŠ¤) ì‚¬ìš©ìë¥¼ ì¬í˜„í•©ë‹ˆë‹¤.\n",
    "# df_user_clean.indexê°€ ë¬¸ìì—´ '2.0'ì„ í¬í•¨í•œë‹¤ê³  ê°€ì •í•˜ê³ , ì¸ë±ìŠ¤ë¥¼ ì§ì ‘ ì§€ì •í•©ë‹ˆë‹¤.\n",
    "example_user_id = 555\n",
    "\n",
    "if example_user_id in df_user_clean.index:\n",
    "    print(f\"\\n\\n{'='*60}\")\n",
    "    print(\n",
    "        f\"       âœ… ì‚¬ìš©ì ID {example_user_id}ì— ëŒ€í•œ ì‹œë‚˜ë¦¬ì˜¤ë³„ ì¶”ì²œ ê²°ê³¼ (intake_timing ê¸°ë°˜)\"\n",
    "    )\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # 1. ìš´ë™ ì „ ì¶”ì²œ (Pre-Workout)\n",
    "    print(\"\\n--- ğŸ‹ï¸ ìš´ë™ ì „ (Pre-Workout) ì¶”ì²œ (ìƒìœ„ 3ê°œ) ---\")\n",
    "    recs_pre = get_filtered_recommendations(\n",
    "        user_id=example_user_id,\n",
    "        model=model,\n",
    "        dataset=timing_datasets,\n",
    "        user_features_matrix=user_features_matrix,\n",
    "        item_features_matrix=item_features_matrix,\n",
    "        df_item_raw=df_item_raw,\n",
    "        timing=\"Pre\",\n",
    "        k_final=3,\n",
    "    )\n",
    "    try:\n",
    "        print(recs_pre.to_markdown(index=False))\n",
    "    except ImportError:\n",
    "        print(recs_pre.to_string(index=False))\n",
    "\n",
    "    # 2. ìš´ë™ ì¤‘ ì¶”ì²œ (Intra-Workout)\n",
    "    print(\"\\n--- ğŸ’§ ìš´ë™ ì¤‘ (Intra-Workout) ì¶”ì²œ (ìƒìœ„ 3ê°œ) ---\")\n",
    "    recs_intra = get_filtered_recommendations(\n",
    "        user_id=example_user_id,\n",
    "        model=model,\n",
    "        dataset=timing_datasets,\n",
    "        user_features_matrix=user_features_matrix,\n",
    "        item_features_matrix=item_features_matrix,\n",
    "        df_item_raw=df_item_raw,\n",
    "        timing=\"Intra\",\n",
    "        k_final=3,\n",
    "    )\n",
    "    try:\n",
    "        print(recs_intra.to_markdown(index=False))\n",
    "    except ImportError:\n",
    "        print(recs_intra.to_string(index=False))\n",
    "\n",
    "    # 3. ìš´ë™ í›„ ì¶”ì²œ (Post-Workout)\n",
    "    print(\"\\n--- ğŸ’ª ìš´ë™ í›„ (Post-Workout) ì¶”ì²œ (ìƒìœ„ 3ê°œ) ---\")\n",
    "    recs_post = get_filtered_recommendations(\n",
    "        user_id=example_user_id,\n",
    "        model=model,\n",
    "        dataset=timing_datasets,\n",
    "        user_features_matrix=user_features_matrix,\n",
    "        item_features_matrix=item_features_matrix,\n",
    "        df_item_raw=df_item_raw,\n",
    "        timing=\"Post\",\n",
    "        k_final=3,\n",
    "    )\n",
    "    try:\n",
    "        print(recs_post.to_markdown(index=False))\n",
    "    except ImportError:\n",
    "        print(recs_post.to_string(index=False))\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "else:\n",
    "    print(\n",
    "        f\"\\nìœ íš¨í•œ ì‚¬ìš©ì ID({example_user_id})ê°€ ì—†ì–´ íƒ€ì´ë°ë³„ ì¶”ì²œì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db74dd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ” íƒ€ì´ë°ë³„ ì œí’ˆ ë¶„í¬ ë””ë²„ê¹…\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š ì „ì²´ ì œí’ˆì˜ timing_category ë¶„í¬:\n",
      "  Pre            :  86ê°œ ì œí’ˆ\n",
      "  Intra          :  42ê°œ ì œí’ˆ\n",
      "  Post           : 120ê°œ ì œí’ˆ\n",
      "  Pre+Intra      :   0ê°œ ì œí’ˆ\n",
      "  Pre+Post       :  15ê°œ ì œí’ˆ\n",
      "  Intra+Post     :   0ê°œ ì œí’ˆ\n",
      "  All three      :   0ê°œ ì œí’ˆ\n",
      "\n",
      "ğŸ“¦ ì´ ì œí’ˆ ìˆ˜: 234\n",
      "\n",
      "ğŸ“ timing_category ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ):\n",
      "  ANIMAL_MEAL_CHOCOLATE          -> ['Post']\n",
      "  CBUM_MASSGAINER_CHOCOLATE      -> ['Post']\n",
      "  CBUM_MASSGAINER_COOKIESCREAM   -> ['Post']\n",
      "  CBUM_MASSGAINER_VANILLA        -> ['Post']\n",
      "  GASPARI_REALMASS_CHOCOLATEMILK -> ['Post']\n",
      "  LABRADA_MUSCLEMASS_CHOCOLATE   -> ['Post']\n",
      "  MHP_UPYOURMASS_CHOCOLATE       -> ['Post']\n",
      "  REDCON_MRE_FUDGEBROWNIE        -> ['Post']\n",
      "  REDCON_MRE_OATMEALCHOCOLATECHI -> ['Post']\n",
      "  REDCON_MRE_PEANUTBUTTERCOOCKIE -> ['Post']\n",
      "\n",
      "ğŸ¯ ê° íƒ€ì´ë°ë³„ ì‚¬ìš© ê°€ëŠ¥í•œ ì œí’ˆ ID ì˜ˆì‹œ (ì²˜ìŒ 5ê°œì”©):\n",
      "\n",
      "  Pre (86ê°œ ì œí’ˆ):\n",
      "    - SAMDAE_INSEASON_CHERRYLEMON\n",
      "    - SAMDAE_INSEASON_PLAIN\n",
      "    - SAMDAE_OFFSEASON_GRAIN\n",
      "    - SAMDAE_OFFSEASON_SWEETPOTATO\n",
      "    - SAMDAE_PRE_GRAPE\n",
      "\n",
      "  Intra (42ê°œ ì œí’ˆ):\n",
      "    - ALLRIGHT_SIGNAL_SOURAPPLE\n",
      "    - ANIMAL_JUICEDAMINO_ORANGEJUICE\n",
      "    - EXTREME_BCAA_TAURINE\n",
      "    - GASPARI_PROVEN_BLUEBERRYACAI\n",
      "    - GASPARI_PROVEN_GUAVANECTARINGE\n",
      "\n",
      "  Post (120ê°œ ì œí’ˆ):\n",
      "    - ANIMAL_MEAL_CHOCOLATE\n",
      "    - CBUM_MASSGAINER_CHOCOLATE\n",
      "    - CBUM_MASSGAINER_COOKIESCREAM\n",
      "    - CBUM_MASSGAINER_VANILLA\n",
      "    - GASPARI_REALMASS_CHOCOLATEMILKSHAKE\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# ğŸ” íƒ€ì´ë°ë³„ ì œí’ˆ ë¶„í¬ í™•ì¸ (ë””ë²„ê¹…)\n",
    "# ================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ” íƒ€ì´ë°ë³„ ì œí’ˆ ë¶„í¬ ë””ë²„ê¹…\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. ì „ì²´ ì œí’ˆì˜ íƒ€ì´ë° ë¶„í¬ í™•ì¸\n",
    "if \"timing_category\" in df_item_raw.columns:\n",
    "    print(\"\\nğŸ“Š ì „ì²´ ì œí’ˆì˜ timing_category ë¶„í¬:\")\n",
    "\n",
    "    # timing_categoryê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸\n",
    "    def count_timing_distribution(df):\n",
    "        pre_count = 0\n",
    "        intra_count = 0\n",
    "        post_count = 0\n",
    "        both_pre_intra = 0\n",
    "        both_pre_post = 0\n",
    "        both_intra_post = 0\n",
    "        all_three = 0\n",
    "\n",
    "        for cat in df[\"timing_category\"]:\n",
    "            if isinstance(cat, list):\n",
    "                if \"Pre\" in cat:\n",
    "                    pre_count += 1\n",
    "                if \"Intra\" in cat:\n",
    "                    intra_count += 1\n",
    "                if \"Post\" in cat:\n",
    "                    post_count += 1\n",
    "\n",
    "                if (\n",
    "                    \"Pre\" in cat\n",
    "                    and \"Intra\" in cat\n",
    "                    and len([x for x in cat if x in [\"Pre\", \"Intra\", \"Post\"]]) == 2\n",
    "                ):\n",
    "                    both_pre_intra += 1\n",
    "                if (\n",
    "                    \"Pre\" in cat\n",
    "                    and \"Post\" in cat\n",
    "                    and len([x for x in cat if x in [\"Pre\", \"Intra\", \"Post\"]]) == 2\n",
    "                ):\n",
    "                    both_pre_post += 1\n",
    "                if (\n",
    "                    \"Intra\" in cat\n",
    "                    and \"Post\" in cat\n",
    "                    and len([x for x in cat if x in [\"Pre\", \"Intra\", \"Post\"]]) == 2\n",
    "                ):\n",
    "                    both_intra_post += 1\n",
    "                if \"Pre\" in cat and \"Intra\" in cat and \"Post\" in cat:\n",
    "                    all_three += 1\n",
    "            else:\n",
    "                if cat == \"Pre\":\n",
    "                    pre_count += 1\n",
    "                elif cat == \"Intra\":\n",
    "                    intra_count += 1\n",
    "                elif cat == \"Post\":\n",
    "                    post_count += 1\n",
    "\n",
    "        return {\n",
    "            \"Pre\": pre_count,\n",
    "            \"Intra\": intra_count,\n",
    "            \"Post\": post_count,\n",
    "            \"Pre+Intra\": both_pre_intra,\n",
    "            \"Pre+Post\": both_pre_post,\n",
    "            \"Intra+Post\": both_intra_post,\n",
    "            \"All three\": all_three,\n",
    "        }\n",
    "\n",
    "    timing_dist = count_timing_distribution(df_item_raw)\n",
    "    for key, val in timing_dist.items():\n",
    "        print(f\"  {key:15s}: {val:3d}ê°œ ì œí’ˆ\")\n",
    "\n",
    "    print(f\"\\nğŸ“¦ ì´ ì œí’ˆ ìˆ˜: {len(df_item_raw)}\")\n",
    "\n",
    "    # 2. ìƒ˜í”Œ timing_category ê°’ í™•ì¸\n",
    "    print(\"\\nğŸ“ timing_category ìƒ˜í”Œ (ì²˜ìŒ 10ê°œ):\")\n",
    "    for idx, row in df_item_raw.head(10).iterrows():\n",
    "        print(f\"  {row[ITEM_ID_COL][:30]:30s} -> {row['timing_category']}\")\n",
    "\n",
    "    # 3. ê° íƒ€ì´ë°ë³„ë¡œ ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ì œí’ˆ ID í™•ì¸\n",
    "    print(\"\\nğŸ¯ ê° íƒ€ì´ë°ë³„ ì‚¬ìš© ê°€ëŠ¥í•œ ì œí’ˆ ID ì˜ˆì‹œ (ì²˜ìŒ 5ê°œì”©):\")\n",
    "    for timing in [\"Pre\", \"Intra\", \"Post\"]:\n",
    "        matching_products = df_item_raw[\n",
    "            df_item_raw[\"timing_category\"].apply(\n",
    "                lambda x: timing in x if isinstance(x, list) else x == timing\n",
    "            )\n",
    "        ]\n",
    "        print(f\"\\n  {timing} ({len(matching_products)}ê°œ ì œí’ˆ):\")\n",
    "        for prod_id in matching_products[ITEM_ID_COL].head(5):\n",
    "            print(f\"    - {prod_id}\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ timing_category ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286eb5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building test user top10 rows:   0%|          | 0/552 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building test user top10 rows:  13%|â–ˆâ–        | 71/552 [00:45<04:56,  1.62it/s]"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 10. Train/Test ì‚¬ìš©ìë³„ Top-3 ì¶”ì²œ ê²°ê³¼ ìƒì„± (Pre/Intra/Post, ë¶€ì›ë£Œ)\n",
    "# ================================================================\n",
    "import os\n",
    "\n",
    "\n",
    "# ìœ í‹¸: ë‹¨ì¼ ìœ ì €ì— ëŒ€í•´ ì‹œë‚˜ë¦¬ì˜¤ë³„ Top-3 í–‰ë“¤ì„ ìƒì„±\n",
    "def build_user_top10_rows(\n",
    "    user_id,\n",
    "    user_set_name,\n",
    "    model,\n",
    "    dataset,\n",
    "    user_features_matrix,\n",
    "    item_features_matrix,\n",
    "    df_item_raw,\n",
    "):\n",
    "    rows = []\n",
    "\n",
    "    # 1) Pre/Intra/Post\n",
    "    for scenario in [\"Pre\", \"Intra\", \"Post\"]:\n",
    "        recs = get_filtered_recommendations(\n",
    "            user_id=user_id,\n",
    "            model=model,\n",
    "            dataset=dataset,\n",
    "            user_features_matrix=user_features_matrix,\n",
    "            item_features_matrix=item_features_matrix,\n",
    "            df_item_raw=df_item_raw,\n",
    "            timing=scenario,\n",
    "            k_total=250,\n",
    "            k_final=10,\n",
    "        )\n",
    "        if not recs.empty:\n",
    "            for rank_idx, (_, r) in enumerate(recs.head(10).iterrows(), start=1):\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"user_set\": user_set_name,\n",
    "                        \"user_id\": user_id,\n",
    "                        \"scenario\": scenario,\n",
    "                        \"rank\": rank_idx,\n",
    "                        \"product_id\": r.get(ITEM_ID_COL),\n",
    "                        \"category\": r.get(\"category\"),\n",
    "                        \"product_name\": r.get(\"product_name\"),\n",
    "                        \"predicted_score\": r.get(\"Predicted_Score\"),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    # 2) ë¶€ì›ë£Œë§Œ ë³„ë„ë¡œ Top-3 (íƒ€ì´ë° ë¬´ê´€)\n",
    "    all_recs = recommend_for_user(\n",
    "        user_id,\n",
    "        model,\n",
    "        dataset,\n",
    "        user_features_matrix,\n",
    "        item_features_matrix,\n",
    "        df_item_raw,\n",
    "        k=250,\n",
    "    )\n",
    "    if not all_recs.empty and \"ingredient_type\" in df_item_raw.columns:\n",
    "        # ë©”íƒ€ë°ì´í„°ì™€ ê²°í•©í•´ì„œ ingredient_type ì ‘ê·¼\n",
    "        merged = all_recs.merge(\n",
    "            df_item_raw[[ITEM_ID_COL, \"ingredient_type\", \"category\", \"product_name\"]],\n",
    "            on=ITEM_ID_COL,\n",
    "            how=\"left\",\n",
    "        )\n",
    "        sub_ing = merged[merged[\"ingredient_type\"] == \"ë¶€ì›ë£Œ\"].head(3)\n",
    "        if not sub_ing.empty:\n",
    "            for rank_idx, (_, r) in enumerate(sub_ing.iterrows(), start=1):\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"user_set\": user_set_name,\n",
    "                        \"user_id\": user_id,\n",
    "                        \"scenario\": \"ë¶€ì›ë£Œ\",\n",
    "                        \"rank\": rank_idx,\n",
    "                        \"product_id\": r.get(ITEM_ID_COL),\n",
    "                        \"category\": r.get(\"category\"),\n",
    "                        \"product_name\": r.get(\"product_name\"),\n",
    "                        \"predicted_score\": r.get(\"Predicted_Score\"),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "# # Train/Test ì‚¬ìš©ì ID ëª©ë¡ ìˆ˜ì§‘\n",
    "# train_users = (\n",
    "#     sorted(df_train_interactions[\"user_id\"].unique())\n",
    "#     if \"df_train_interactions\" in globals()\n",
    "#     else []\n",
    "# )\n",
    "test_users = (\n",
    "    sorted(df_test_interactions[\"user_id\"].unique())\n",
    "    if \"df_test_interactions\" in globals()\n",
    "    else []\n",
    ")\n",
    "\n",
    "# print(f\"Train users: {len(train_users)} | Test users: {len(test_users)}\")\n",
    "\n",
    "# # ë°°ì¹˜ ìƒì„±\n",
    "# train_rows = []\n",
    "# for uid in train_users:\n",
    "#     train_rows.extend(\n",
    "#         build_user_top10_rows(\n",
    "#             uid,\n",
    "#             \"train\",\n",
    "#             model,\n",
    "#             timing_datasets,\n",
    "#             user_features_matrix,\n",
    "#             item_features_matrix,\n",
    "#             df_item_raw,\n",
    "#         )\n",
    "#     )\n",
    "\n",
    "test_rows = []\n",
    "from tqdm import tqdm\n",
    "\n",
    "for uid in tqdm(test_users, desc=\"Building test user top10 rows\"):\n",
    "    test_rows.extend(\n",
    "        build_user_top10_rows(\n",
    "            uid,\n",
    "            \"test\",\n",
    "            model,\n",
    "            timing_datasets,\n",
    "            user_features_matrix,\n",
    "            item_features_matrix,\n",
    "            df_item_raw,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì •ë¦¬\n",
    "# train_top10_df = pd.DataFrame(\n",
    "#     train_rows,\n",
    "#     columns=[\n",
    "#         \"user_set\",\n",
    "#         \"user_id\",\n",
    "#         \"scenario\",\n",
    "#         \"rank\",\n",
    "#         \"product_id\",\n",
    "#         \"category\",\n",
    "#         \"product_name\",\n",
    "#         \"predicted_score\",\n",
    "#     ],\n",
    "# )\n",
    "test_top10_df = pd.DataFrame(\n",
    "    test_rows,\n",
    "    columns=[\n",
    "        \"user_set\",\n",
    "        \"user_id\",\n",
    "        \"scenario\",\n",
    "        \"rank\",\n",
    "        \"product_id\",\n",
    "        \"category\",\n",
    "        \"product_name\",\n",
    "        \"predicted_score\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# ì €ì¥ ê²½ë¡œ\n",
    "out_dir = os.path.dirname(os.path.abspath(\"recommendations_top10_train.csv\"))\n",
    "# train_path = os.path.join(out_dir, \"recommendations_top10_train.csv\")\n",
    "test_path = os.path.join(out_dir, \"recommendations_top10_test.csv\")\n",
    "\n",
    "# train_top10_df.to_csv(train_path, index=False)\n",
    "test_top10_df.to_csv(test_path, index=False)\n",
    "\n",
    "print(\"\\n[Saved]\")\n",
    "# print(f\"- Train Top-3: {train_path}\")\n",
    "print(f\"- Test  Top-3: {test_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b3ad33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ“Š ì¶”ì²œ ê²°ê³¼ ë‹¤ì–‘ì„± ë¶„ì„ (ìˆœìœ„ ì •ë³´ í¬í•¨)\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_top10_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# ì‹œë‚˜ë¦¬ì˜¤ë³„ ì œí’ˆë³„ ì¶”ì²œ ë¹ˆë„\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m scenario \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mPre\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mIntra\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPost\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m scenario \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtest_top10_df\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mscenario\u001b[39m\u001b[33m\"\u001b[39m].values:\n\u001b[32m     11\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     13\u001b[39m     scenario_df = test_top10_df[test_top10_df[\u001b[33m\"\u001b[39m\u001b[33mscenario\u001b[39m\u001b[33m\"\u001b[39m] == scenario]\n",
      "\u001b[31mNameError\u001b[39m: name 'test_top10_df' is not defined"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# ë‹¤ì–‘ì„± ë¶„ì„: ì¶”ì²œ ê²°ê³¼ì˜ ë‹¤ì–‘ì„± í™•ì¸ (ìˆœìœ„ ì •ë³´ í¬í•¨)\n",
    "# ================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“Š ì¶”ì²œ ê²°ê³¼ ë‹¤ì–‘ì„± ë¶„ì„ (ìˆœìœ„ ì •ë³´ í¬í•¨)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ì‹œë‚˜ë¦¬ì˜¤ë³„ ì œí’ˆë³„ ì¶”ì²œ ë¹ˆë„\n",
    "for scenario in [\"Pre\", \"Intra\", \"Post\"]:\n",
    "    if scenario not in test_top10_df[\"scenario\"].values:\n",
    "        continue\n",
    "\n",
    "    scenario_df = test_top10_df[test_top10_df[\"scenario\"] == scenario]\n",
    "    product_counts = scenario_df[\"product_id\"].value_counts()\n",
    "\n",
    "    print(f\"\\n--- {scenario} ì¶”ì²œ ({len(scenario_df)}ê±´) ---\")\n",
    "    print(f\"ê³ ìœ  ì œí’ˆ ìˆ˜: {len(product_counts)}ê°œ\")\n",
    "    print(f\"ìƒìœ„ 5ê°œ ì œí’ˆ:\")\n",
    "    for i, (product, count) in enumerate(product_counts.head(5).items(), 1):\n",
    "        percentage = (count / len(scenario_df)) * 100\n",
    "        product_name = (\n",
    "            scenario_df[scenario_df[\"product_id\"] == product][\"product_name\"].iloc[0]\n",
    "            if len(scenario_df[scenario_df[\"product_id\"] == product]) > 0\n",
    "            else \"N/A\"\n",
    "        )\n",
    "\n",
    "        # ìˆœìœ„ë³„ ì¶”ì²œ íšŸìˆ˜ ê³„ì‚°\n",
    "        rank_counts = (\n",
    "            scenario_df[scenario_df[\"product_id\"] == product][\"rank\"]\n",
    "            .value_counts()\n",
    "            .sort_index()\n",
    "        )\n",
    "        rank_str = \", \".join([f\"{rank}ìœ„:{cnt}íšŒ\" for rank, cnt in rank_counts.items()])\n",
    "\n",
    "        print(f\"  {i}. {product[:30]:30s} {count:4d}íšŒ ({percentage:5.2f}%)\")\n",
    "        print(f\"      ìˆœìœ„ ë¶„í¬: {rank_str}\")\n",
    "        print(f\"      ì œí’ˆëª…: {product_name[:40]}\")\n",
    "\n",
    "    # ë‹¤ì–‘ì„± ì§€í‘œ\n",
    "    unique_products = len(product_counts)\n",
    "    total_recommendations = len(scenario_df)\n",
    "    diversity = (unique_products / total_recommendations) * 100\n",
    "    print(\n",
    "        f\"\\n  ë‹¤ì–‘ì„± ì§€í‘œ: {unique_products}/{total_recommendations} = {diversity:.2f}%\"\n",
    "    )\n",
    "\n",
    "    # 1ìœ„ë¡œ ì¶”ì²œëœ íšŸìˆ˜ ê¸°ì¤€ ì§‘ì¤‘ë„\n",
    "    rank1_df = scenario_df[scenario_df[\"rank\"] == 1]\n",
    "    rank1_counts = rank1_df[\"product_id\"].value_counts()\n",
    "    rank1_percentage = (\n",
    "        (rank1_counts.iloc[0] / len(rank1_df)) * 100 if len(rank1_counts) > 0 else 0\n",
    "    )\n",
    "    print(f\"  1ìœ„ ì¶”ì²œ ì§‘ì¤‘ë„: {rank1_counts.iloc[0]}íšŒ ({rank1_percentage:.2f}%)\")\n",
    "\n",
    "    # ìƒìœ„ 3ê°œ ì œí’ˆì´ ì°¨ì§€í•˜ëŠ” ë¹„ìœ¨\n",
    "    top3_percentage = (product_counts.head(3).sum() / total_recommendations) * 100\n",
    "    print(f\"  ìƒìœ„ 3ê°œ ì œí’ˆì˜ ì´ ì ìœ ìœ¨: {top3_percentage:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… ë…¸íŠ¸ë¶ ì¬ì‹¤í–‰ ì‹œ ë‹¤ì–‘ì„± ê°œì„  íš¨ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9ff665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ” ìƒí˜¸ì‘ìš© ë°ì´í„° ì œí’ˆ ë¹ˆë„ ë¶„ì„\n",
      "================================================================================\n",
      "âŒ df_interactionsê°€ ì—†ìŠµë‹ˆë‹¤!\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# ğŸ” ìƒí˜¸ì‘ìš© ë°ì´í„° ì œí’ˆ ë¹ˆë„ í™•ì¸\n",
    "# ================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ” ìƒí˜¸ì‘ìš© ë°ì´í„° ì œí’ˆ ë¹ˆë„ ë¶„ì„\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if \"df_interactions\" in globals():\n",
    "    # ì „ì²´ ìƒí˜¸ì‘ìš© ë°ì´í„° ì œí’ˆ ë¹ˆë„\n",
    "    print(\"\\nğŸ“Š ì „ì²´ ìƒí˜¸ì‘ìš© ë°ì´í„° ì œí’ˆ ë¹ˆë„ (ìƒìœ„ 20ê°œ):\")\n",
    "    item_counts_all = df_interactions[\"item_id\"].value_counts()\n",
    "    for i, (item_id, count) in enumerate(item_counts_all.head(20).items(), 1):\n",
    "        percentage = (count / len(df_interactions)) * 100\n",
    "        print(f\"  {i:2d}. {item_id[:45]:45s} {count:4d}íšŒ ({percentage:5.2f}%)\")\n",
    "\n",
    "    print(f\"\\nğŸ“¦ ì „ì²´ ìƒí˜¸ì‘ìš© ë°ì´í„°: {len(df_interactions)}ê±´\")\n",
    "    print(f\"ğŸ“¦ ê³ ìœ  ì œí’ˆ ìˆ˜: {len(item_counts_all)}ê°œ\")\n",
    "    print(f\"ğŸ“¦ ì‚¬ìš©ì ìˆ˜: {df_interactions['user_id'].nunique()}ëª…\")\n",
    "\n",
    "    # íƒ€ì´ë°ë³„ë¡œ ì œí’ˆì„ í•„í„°ë§í•˜ì—¬ í™•ì¸\n",
    "    if \"df_item_raw\" in globals() and \"timing_category\" in df_item_raw.columns:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ğŸ¯ íƒ€ì´ë°ë³„ ì‹¤ì œ ìƒí˜¸ì‘ìš© ì œí’ˆ ë¹ˆë„\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "        # íƒ€ì´ë°ë³„ ì œí’ˆ ID ê°€ì ¸ì˜¤ê¸°\n",
    "        for timing in [\"Pre\", \"Intra\", \"Post\"]:\n",
    "            timing_items = df_item_raw[\n",
    "                df_item_raw[\"timing_category\"].apply(\n",
    "                    lambda x: timing in x if isinstance(x, list) else x == timing\n",
    "                )\n",
    "            ][ITEM_ID_COL].unique()\n",
    "\n",
    "            timing_interactions = df_interactions[\n",
    "                df_interactions[\"item_id\"].isin(timing_items)\n",
    "            ]\n",
    "\n",
    "            print(f\"\\n--- {timing} ìƒí˜¸ì‘ìš© (ì´ {len(timing_interactions)}ê±´) ---\")\n",
    "            timing_item_counts = timing_interactions[\"item_id\"].value_counts()\n",
    "\n",
    "            print(f\"ê³ ìœ  ì œí’ˆ ìˆ˜: {len(timing_item_counts)}ê°œ\")\n",
    "            print(f\"ìƒìœ„ 10ê°œ ì œí’ˆ:\")\n",
    "            for i, (item_id, count) in enumerate(\n",
    "                timing_item_counts.head(10).items(), 1\n",
    "            ):\n",
    "                percentage = (\n",
    "                    (count / len(timing_interactions)) * 100\n",
    "                    if len(timing_interactions) > 0\n",
    "                    else 0\n",
    "                )\n",
    "                print(f\"  {i:2d}. {item_id[:45]:45s} {count:4d}íšŒ ({percentage:5.2f}%)\")\n",
    "\n",
    "            # ì§‘ì¤‘ë„ ì§€í‘œ\n",
    "            if len(timing_item_counts) > 0:\n",
    "                top1_percentage = (\n",
    "                    (timing_item_counts.iloc[0] / len(timing_interactions)) * 100\n",
    "                    if len(timing_interactions) > 0\n",
    "                    else 0\n",
    "                )\n",
    "                top10_percentage = (\n",
    "                    (timing_item_counts.head(3).sum() / len(timing_interactions)) * 100\n",
    "                    if len(timing_interactions) > 0\n",
    "                    else 0\n",
    "                )\n",
    "                print(f\"\\n  ìƒìœ„ 1ê°œ ì ìœ ìœ¨: {top1_percentage:.2f}%\")\n",
    "                print(f\"  ìƒìœ„ 3ê°œ ì ìœ ìœ¨: {top10_percentage:.2f}%\")\n",
    "else:\n",
    "    print(\"âŒ df_interactionsê°€ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e41651f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightfm_python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
